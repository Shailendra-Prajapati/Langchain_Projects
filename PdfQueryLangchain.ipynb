{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF Query Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\skp16\\anaconda3\\lib\\site-packages (0.0.200)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.9 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (0.0.10)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (0.5.8)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (1.10.9)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (22.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: openai in c:\\users\\skp16\\anaconda3\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.6/232.6 kB 6.9 MB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 10.8/10.8 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-win_amd64.whl (635 kB)\n",
      "     -------------------------------------- 635.3/635.3 kB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\skp16\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain\n",
    "! pip install openai\n",
    "! pip install PyPDF2\n",
    "! pip install faiss-cpu\n",
    "! pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]= \"Use You OpenAI API Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path of the pdf file/files\n",
    "\n",
    "pdfreader = PdfReader('AI For Metaverse.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "\n",
    "# Read text from Pdf\n",
    "\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content= page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\nArtiﬁcial Intelligence for the Metaverse: A Survey\\nThien Huynh-The, Member, IEEE, Quoc-Viet Pham, Member, IEEE, Xuan-Qui Pham,\\nThanh Thi Nguyen, Zhu Han, Fellow, IEEE, and Dong-Seong Kim, Senior, IEEE\\nAbstract —Along with the massive growth of the Internet\\nfrom the 1990s until now, various innovative technologies have\\nbeen created to bring users breathtaking experiences with more\\nvirtual interactions in cyberspace. Many virtual environments\\nwith thousands of services and applications, from social networks\\nto virtual gaming worlds, have been developed with immersive\\nexperience and digital transformation, but most are incoherent\\ninstead of being integrated into a platform. In this context,\\nmetaverse, a term formed by combining meta and universe, has\\nbeen introduced as a shared virtual world that is fueled by many\\nemerging technologies, such as ﬁfth-generation networks and be-\\nyond, virtual reality, and artiﬁcial intelligence (AI). Among such\\ntechnologies, AI has shown the great importance of processing\\nbig data to enhance immersive experience and enable human-\\nlike intelligence of virtual agents. In this survey, we make a\\nbeneﬁcial effort to explore the role of AI in the foundation and\\ndevelopment of the metaverse. We ﬁrst deliver a preliminary\\nof AI, including machine learning algorithms and deep learning\\narchitectures, and its role in the metaverse. We then convey a\\ncomprehensive investigation of AI-based methods concerning six\\ntechnical aspects that have potentials for the metaverse: natural\\nlanguage processing, machine vision, blockchain, networking,\\ndigital twin, and neural interface, and being potential for the\\nmetaverse. Subsequently, several AI-aided applications, such as\\nhealthcare, manufacturing, smart cities, and gaming, are studied\\nto be deployed in the virtual worlds. Finally, we conclude the\\nkey contribution of this survey and open some future research\\ndirections in AI for the metaverse.\\nIndex Terms —3D virtual world, artiﬁcial intelligence, deep\\nlearning, machine learning, metaverse.\\nI. I NTRODUCTION\\nSINCE Facebook rebranded itself as Meta, announced by\\nMark Zuckerberg in October 2021, the marvelous concept\\nregarding the new name has become a hot trend on social\\nmedia and received huge attention and much more discussions\\nby various communities, including academia and industry.\\nBesides Meta, some big tech companies have some metaverse\\ninvestment and development activities, such as Microsoft\\nbought Activision Blizzard, a video game holding company,\\nfor$68 :7billion as the deal of gaming expansion into the\\nmetaverse. Recently, Metaverse Group, a metaverse real estate\\ninvestment company bought a parcel of land on a decentralized\\nvirtual reality platform known as Decentraland for a shocking\\nThien Huynh-Them, Xuan-Qui Pham, and Dong-Seong Kim are with\\nthe Department of IT Convergence, Kumoh National Institute of Tech-\\nnology, Gumi, Gyeongsangbuk-do 39177, Republic of Korea (email:\\nthienht@kumoh.ac.kr, pxuanqui@kumoh.ac.kr, dskim@kumoh.ac.kr).\\nQuoc-Viet Pham is with the Korean Southeast Center for the 4th Indus-\\ntrial Revolution Leader Education, Pusan National University, Busan 46241,\\nRepublic of Korea (email: vietpq@pusan.ac.kr).\\nThanh Thi Nguyen is with the School of Information Technol-\\nogy, Deakin University, Waurn Ponds, VIC 3216, Australia (email:\\nthanh.nguyen@deakin.edu.au).\\nZhu Han is with the Department of Electrical and Computer Engineering,\\nUniversity of Houston, Houston, TX 77004 USA (e-mail: zhan2@uh.edu).\\nSoftware &\\nservice\\n$183.3BAR/VR device\\n$4.7B\\nIn-game ads\\n$31.8BGame\\nhardware\\n$63.5B\\n2020 2024Software &\\nservice\\n$250.5BIn-game ads\\n$53.7BGame\\nhardware\\n$77.8BAR/VR device\\n$40.6BFig. 1. Gaming revenue growth aided by 3D virtual worlds.\\nprice $2:43million, and recorded as the highest ever amount\\nfor a virtual real estate. A famous rapper who bought a plot\\nof land in the Sandbox metaverse for $450 ;000 is Snoop\\nDogg, in which this rapper can hold virtual events like music\\nfestivals and concerts to bring an immersive experience to\\nthe audience participating in the virtual world via the virtual\\nreality technology. In the near future, the metaverse is realized\\nas the next big technology and currently attracting online\\ngame makers, internet ﬁnance businesses, social networks,\\nand other technology leaders. The Seoul metropolitan govern-\\nment just very recently announced a plan called Metaverse\\nSeoul that creates a virtual communication ecosystem for\\nall municipal administrative areas, such as culture, tourism,\\neconomic, educational, and civic service. Besides providing\\ndifferent business support services and facilities, the Metaverse\\nSeoul will offer some specialized services for people with\\ndisabilities to take pleasure in safety and convenience contents\\nusing extended reality (XR) technology. Based on the analysis\\nof Bloomberg Intelligence [1], the global metaverse revenue\\nopportunity will increase from USD 500 billion in 2020 to\\nUSD 800billion in 2024, in which the online game industry\\nwill take half of the global revenue. Remarkably, the video\\ngame companies and studios have some plans to upgrade\\nexisting traditional games to three-dimensional (3D) virtual\\nworld convolving social networks, in which some attractive\\nactivities, such as live entertainment and media advertising\\nevents, can be held besides gaming. In Fig. 1, the revenue\\nof virtual reality (VR) hardware and in-game advertisement\\nsigniﬁcantly increases through the advancement of virtual\\nactivities in the metaverse.\\nThe metaverse is not a new idea because it has circulated\\nalong with the development of the Internet and other tech-\\nnologies for decades. Fig. 2 describes the timeline of the\\nmetaverse development that involves many primary events,\\nfrom the birth of the Internet and the ﬁrst mention in liter-\\nature to the ﬁrst virtual world project with Second Life andarXiv:2202.10336v1  [cs.CY]  15 Feb 20222\\n1991Birth of the\\nInternet\\n1992\\nTerm \"metaverse\"\\nin Snow Crash\\nProof of work\\nB-Money1993 1998\\n2017 20162002\\n20152006\\n2011\\n2015 2021 2016Digital\\nTwins\\nOnline virtual\\nworld2009 2003\\nBitcoin\\nReady\\nplayer one\\nCreative\\ngrame\\n2009\\nBlockchain\\ntechnology\\nVR in novel\\n2012\\n2014\\nNFT - non-\\nfungible token\\nOculus - VR\\nhardware\\nEthereum\\nnetworkDecentraland\\nPokemon GO DAODecentralized\\nAutonomous\\nOrganization\\nMultiplayer game\\nand social hub2018Virtual game with\\ntraining and\\ntrading creatures\\n2021\\nMicrosoft\\nMeshMeta Platform\\n(formerly known\\nas Facebook)\\nFig. 2. A timeline of the metaverse development involving primary events from 1991 to 2021.\\nrecent metaverse projects of big tech companies like Microsoft\\nand Facebook. Metaverse is the term formed by combining\\nMeta and Universe [2], which may be ﬁrst mentioned in\\nthe dystopian cyberpunk novel Snow Crash in 1992 to de-\\nscribe a virtual reality world called the matrix. At present,\\nthe metaverse is deﬁned as a shared virtual 3D world or\\neven multiple cross-platform worlds that can provide users\\na comprehensively immersive experience with interactive and\\ncollaborative activities. Besides virtual places and construc-\\ntions ﬁxed in the virtual world, many other entities, such as\\nobjects, user identities, and digital goods, can be exchanged\\nbetween different virtual worlds and even reﬂected into the\\nreality world [3]. Few recent years have witnessed an un-\\nprecedented explosion of the metaverse, mostly derived from\\n3D gaming, which is fueled by the improvement of hardware\\n(e.g., big data storage infrastructure, wireless communication\\nnetworks, built-in sensors, and graphic processing unit - GPU)\\nand the optimization of software (e.g., resource allocation in\\ncommunications, language processing, and computer vision) to\\nbuild the virtual world more solidly and creatively. Different\\nfrom the traditional metaverse modality that limits immersive\\nexperience poorly by insufﬁcient data, the new one not only\\ngenerates a huge new source of user and behavioral data\\nfor enterprises (where users freely make creative content) but\\nalso presents a plentiful foundation to deploy artiﬁcial intel-\\nligence (AI) into various domains, such as natural language\\nprocessing, computer vision, and neural interface. Besides,\\na standard platform built for a modern metaverse should\\nsatisfy the following characteristics: virtual world, persistency,\\nscalability, always-on with synchronicity, ﬁnancial allowance,\\ndecentralization, security, and interoperability. In [4], a meta-\\nverse platform can include several layers (see Fig. 3) which\\nare expressed as follows:\\n\\x0fInfrastructure : 5G, 6G, WiFi, cloud, data center, central\\nprocessing units, and GPUs.\\n\\x0fHuman interface : mobile, smartwatch, smartglasses,\\nwearable devices, head-mounted display, gestures, voice,\\nHuman\\nInterfaceDecentralizationSpatial\\nComputingCreator\\nEconomyDiscoveryExperience\\nInfrastructureFig. 3. Seven layers of a metaverse platform.\\nand electrode bundle.\\n\\x0fDecentralization : edge computing, AI agents, blockchain,\\nand microservices.\\n\\x0fSpatial computing : 3D engines, VR, augmented reality\\n(AR), XR, geospatial mapping, and multitasking.\\n\\x0fCreator economy : design tools, asset markets, E-\\ncommerce, and workﬂow.\\n\\x0fDiscovery : advertising networks, virtual stores, social\\ncuration, ratings, avatar, and chatbot.\\n\\x0fExperience : games, social, E-sports, shopping, festivals,\\nevents, learning, and working.\\nIt is not hard to ﬁnd out the presence of AI inside\\nlayers, with machine learning (ML) algorithms and deep\\nlearning (DL) architectures, along with their importance in\\nmany diversiﬁed aspects. For instance, many ML algorithms\\nwith supervised and unsupervised learning were applied in\\nclassiﬁcation and regression models for voice recognition and\\nother language processing tasks that enable system agents to\\nunderstand user commands. With the input data of sensor-\\nbased signals collected by multiple devices, such as mobile,3\\nsmartwatch, and other wearable devices, the complex patterns\\nof human actions can be analyzed and learned for some\\napplications like physical activity recognition that allows a\\nsystem to perceive user activities and interactions in the virtual\\nworld. Recently, DL has emerged as a powerful AI tool to\\ndeal with the practical issue of understanding complicated\\npatterns from large-messy-confusing data. With considerable\\nsuccess in the computer vision domain, DL is now being\\nleveraged in different domains, such as wireless communi-\\ncations, human-computer interaction, gaming, and ﬁnance.\\nA few years ago, NVIDIA introduced DL super sampling\\n(DLSS), a groundbreaking technology that exploits the power\\nof DL and other AI algorithms to boost the frame rate while\\nmaintaining beautiful and sharp in-game images, thus being\\npotential to improve the visual experience in the metaverse. AI\\nwas also leveraged to improve game balance in several online\\nmultiplayer games by training supervised learning models\\niteratively until satisfying designers and play-testers. To dive\\ninto a new era of 3D design simulation and collaboration for\\ncreating an impressive virtual-reality world, in the metaverse,\\nas rich as the real world, NVIDIA introduced Omniverse1, an\\nopen-and-extensible platform, owning many valuable features,\\nincluding physically-accurate simulation, multi-user design\\ncollaboration, photorealistic and real-time rendering, and AI-\\naccelerated workﬂows.\\nA. Our Contributions\\nThe metaverse platform is built by merging many advanced\\ntechnologies to bring a completely 3D immersive experience\\nto users, where they can truly interact and collaborate with\\nothers in the virtual worlds. Among such technologies like\\nblockchain, XR/VR, and 5G, AI has a silent but important\\nrole in the foundation and development of the metaverse.\\nHowever, understanding how AI can affect and contribute\\nto the metaverse in the technical and application aspects\\nis dubious, especially in the context of which it is neither\\nmentioned in a fancy way like XR/VR nor discussed gloriously\\non social media like blockchain. No existing work is done to\\nprovide a comprehensive review of the role and use of AI in\\nthe metaverse.\\nIn the paper, we convey a comprehensive survey of the\\nexisting AI-based works in the technical and application\\nperspectives and further discuss their potentials for the meta-\\nverse. In a nutshell, the main contributions of this paper are\\nsummarized as follows.\\n\\x0fWe brieﬂy review AI techniques, including conventional\\nML algorithms and innovative DL architectures, with\\nvarious learning strategies like supervised learning, unsu-\\npervised learning, and reinforcement learning. Based on\\nthat, the role of AI in the metaverse is initially revealed.\\n\\x0fWe survey the state-of-the-art AI-powered approaches in\\nsix technical aspects, including natural language process-\\ning, machine vision, blockchain, networking, digital twin\\n(DT), and neural interface, which show great potential\\nfor the metaverse platform.\\n1https://www.nvidia.com/en-us/omniverse/\\x0fWe investigate the existing AI-aided methods relying on\\nseveral application aspects, such as healthcare, gaming,\\nmanufacturing, smart cities, E-commerce, real estate, and\\ndecentralized ﬁnance, which receive more interest to be\\ndeployed in the virtual world.\\n\\x0fWe introduce several interesting metaverse projects which\\nhave applied AI to enhance the immersive experience and\\ndevelop user-oriented services. Moreover, some future\\nresearch directions in AI for the metaverse are discussed.\\nB. Paper Organization\\nThe remaining of this paper is organized as follows.\\nThe fundamentals of blockchain, metaverse, and the role of\\nblockchain in the metaverse are presented in Section II. The\\nadoptions of AI for the metaverse in technical aspects, such\\nas natural language processing, machine vision, blockchain,\\nnetworking, DT, and neural interface, are investigated in Sec-\\ntion III. In Section IV, we discuss several AI-powered applica-\\ntions to be promisingly developed in the metaverse, including\\nhealthcare, manufacturing, smart cities, and gaming, besides\\nother minor areas as E-commerce, human resources, real\\nestate, and decentralized ﬁnance. Some remarkable metaverse\\nprojects are presented in Section V. Finally, we conclude the\\npaper with some future research directions for the development\\nof the metaverse in Section VI.\\nII. AI FOR THEMETAVERSE : PRELIMINARIES\\nThis section brieﬂy conveys a wide spectrum of AI, from\\ntraditional ML algorithms to advanced DL networks that\\nembrace different learning mechanisms, and then articulates\\nthe role of AI in the metaverse.\\nA. Categorization of AI\\nThis part reviews some common AI/ML algorithms that\\nare potential for the metaverse. Fundamentally, most of the\\nexisting AI/ML algorithms can be categorized into two sectors:\\nconventional techniques and advanced techniques, which are\\nstudied for three principal problems: clustering, classiﬁcation,\\nand regression.\\n1) Conventional Techniques: Conventional AI/ML algo-\\nrithms can be grouped based on the kinds of data available for\\nthe learning model: supervised learning, unsupervised learn-\\ning, semi-supervised learning, and reinforcement learning.\\nSupervised Learning: The ML algorithms of this learning\\napproach learn the relation between input and output via a\\nmapping function using labeled data. Each input sample in\\na training dataset is tagged with the answer (a.k.a., label),\\nwhich allows the trained model to classify or predict the\\noutcome for an unforeseen input sample [5]. Supervised\\nlearning algorithms are usually used to handle classiﬁcation\\nproblems (assign a sample in the test set into a discrete class)\\nand regression problems (express the relationship between\\ndependent and independent variables in continuous data).\\nSome regular supervised learning algorithms are decision tree,\\nrandom forest, Naive Bayes, k-nearest neighbor, and support\\nvector machine (SVM).4\\nUnsupervised Learning: Unsupervised learning involves the\\nutilization of AI/ML algorithms for unlabeled data analysis\\nand clustering. These algorithms cannot be applied to classi-\\nﬁcation and regression problems directly, but they are capable\\nof modeling hidden patterns and ﬁnding out data groups\\nwithout the need for human intervention. Unsupervised learn-\\ning algorithms (for example, hierarchical clustering, k-means\\nclustering, principal component analysis, and association rule)\\ncan be used for some common tasks of data mining, such as\\nclustering, association, and dimensionality reduction [6].\\nSemi-supervised Learning: Semi-supervised learning is in-\\ntroduced to partly counter the disadvantages of supervised\\nlearning (e.g., expensive cost for labeling data by scientists\\nand ML engineers) and unsupervised learning (e.g., the lim-\\nitation of application spectrum). In semi-supervised learning,\\nan AI model is trained upon the combination of labeled and\\nunlabeled data. A basic procedure of this type of learning\\ninvolves two steps: clustering similar data using an unsuper-\\nvised learning algorithm and then using the existing labeled\\ndata to label the remaining unlabeled data [7]. Some well-\\nknown algorithms for semi-supervised learning are graph-\\nbased model, generative model, boosting, and self-training [8].\\nReinforcement Learning Reinforcement learning (RL) is a\\ngroup of ML algorithms for making a sequence of decisions,\\nin which an agent learns to attain a goal in an uncertain\\nand complex environment [9]. An AI machine should come\\nthrough trial and error to reach a nearly optimal solution for\\na game-like scenario. The object of an RL model is how to\\nperform the task to maximize the reward and minimize the\\npenalty, beginning with totally random trials and ending with\\nsophisticated tactics and superhuman skills [10]. By exploiting\\nthe power of the searching scheme with many trials, RL is one\\nof the most effective ways to imply machine creativity.\\n2) Advanced Techniques: DL, a subset of AI and ML\\nthat develops multi-layered artiﬁcial neural networks to attain\\nstate-of-the-art accuracy in many classiﬁcation and regression\\ntasks, has been exploited for various applications in multiple\\ndomains [11]–[13]. Unlike traditional ML techniques, DL can\\nautomatically learn underlying features of unstructured data\\nwithout human intervention or human domain knowledge. The\\nhighly ﬂexible architectures of DL allow learning systems to\\nprocess raw data directly and improve learning performance\\nwhen the data is provided enough. Here we discover some\\nwell-known deep architectures, including recurrent neural\\nnetwork (RNN), convolutional neural network (CNN), self-\\norganizing map (SOM), and autoencoders.\\nRecurrent neural network: RNN is one of the founda-\\ntional neural network architectures from which various deep\\narchitectures, such as long short-term memory (LSTM) and\\ngated recurrent unit (GRU) networks, are developed with some\\nstructural improvements. Besides feed-forward connections\\nin regular multilayer networks, an RNN has some feedback\\nconnections associated with the preceding layers. The com-\\nputing ﬂow derived by the feedback connections allows RNNs\\nto maintain memory of past inputs and process models in\\ntime [14]. RNNs can be unfolded in time and trained with\\nback-propagation mechanisms.\\nConvolutional neural network As one of the most suc-cessful deep network architectures, CNN leverages princi-\\nples from linear algebra (especially matrix multiplication) to\\nidentify complex patterns from high-dimensional unstructured\\ndata [15]. Early layers compute features from coarse to ﬁne\\nin a regular CNN, and later layers recombine these features\\ninto higher-level representations. CNNs are distinguished from\\nother deep network architectures by their superior performance\\nwith different data types, including images, videos, audio sig-\\nnals, and communication signals. There are three main layers\\nin a CNN: convolutional layers for feature extraction, pool-\\ning layers for dimensionality reduction, and fully-connected\\nlayers for classiﬁcation. Several standard architectures were\\nintroduced to solve various challenging tasks in the computer\\nvision domain: AlexNet, VGG, GoogleNet, ResNet, DenseNet,\\nInception, and EfﬁcientNet.\\nSelf-organizing map: SOM is an unsupervised neural net-\\nwork to ﬁnd clusters of the input data points by reducing its\\ndimensionality [16]. In common SOM architectures, weights\\nserve as a characteristic of the node. At the beginning, the\\ninputs are normalized and then randomly chosen to feed the\\nnetwork. Random weights close to zero are associated with\\neach feature of the input record, which represents the input\\nnode. The node with the least Euclidean distance (between\\neach of the output nodes and the input node) is recognized\\nas the most accurate input representation and denoted as the\\nbest matching unit (BMU). By establishing these BMUs as\\ncentroid, other units are calculated similarly and assigned to\\nthe shortest distance cluster.\\nAutoencoder: An autoencoder is a special type of neural\\nnetworks, which is trained with the compression and decom-\\npression functions to map its input to output [17]. In an au-\\ntoencoder network, the input layer is encoded into the hidden\\nlayer using an encoding function for compression, where the\\nnumber of hidden nodes is much less than the number of\\ninput nodes. Accordingly, this hidden layer contains the com-\\npressed representation of the original input. The output layer\\naims to reconstruct a decoding function for decompression\\ninput information. The difference between the input and the\\nreconstructed output in the training phase is calculated using\\nan error function. Since autoencoders can learn continuously\\nwith backward propagation, they are usually applied for self-\\nsupervised learning tasks [18].\\nB. Role of AI in the Metaverse\\nBy merging AI with other technologies, such as AR/VR,\\nblockchain, and networking, the metaverse can create secure,\\nscalable, and realistic virtual worlds on a reliable and always-\\non platform. According to the seven-layer metaverse platform,\\nit is undoubted to realize the important role of AI to guarantee\\nthe reliability of infrastructure and improve its performance\\nso far. In the 5G and future 6G systems, many advanced ML\\nalgorithms with supervised learning and reinforcement learn-\\ning have been adopted for different challenging tasks, such as\\nefﬁcient spectrum monitoring, automatic resource allocation,\\nchannel estimation, trafﬁc off-loading, attack prevention, and\\nnetwork fault detection. With sensor-based wearable devices\\nand other human-machine interaction gadgets, simple human5\\nMetaverse\\'s\\nTechniquesLanguage modeling\\nWord prediction\\nText-to-speech processing\\nSemantic labeling\\nObject detection and segmentation\\nImage restoration and enhancement\\nPose estimation and action recognition\\nVirtual reality\\nAugmented reality\\nMixed realityNLP\\nMachine\\nVision\\nBlockchain\\nData collection and sharing\\nData storage and management\\nData security and privacyUltra-reliable and low-latency communications\\nMulti-access edge computing\\nIntelligent spectrum utilizationData-driven modeling\\nPhysical-digital view integration\\nAnalysis-monitoring-prediction-simulationBrain-computer interface\\nInvasive and non-invasive signals\\nMental state analysis\\nNetworkingDigital\\nTwinNeural\\nInterface\\nFig. 4. Primary technical aspects in the metaverse, in which AI with ML algorithms and DL architectures is advancing the user experience in the virtual\\nworld.\\nmovements and complex actions can be analyzed and rec-\\nognized based on learning ML and DL models. Therefore,\\nusers’ movements in the real world are projected into the\\nvirtual worlds, allowing users to fully control their avatars\\nto interact with other objects in the metaverse comfortably.\\nMoreover, these avatars can engage with many modalities\\nadopted in the real world, such as facial expressions, emotions,\\nbody movement, and physical interactions, besides speech\\nrecognition and sentiment analysis, which are powered by AI\\nin terms of accuracy and processing speed.\\nAlthough XR/VR someway represents the facade of a\\nmetaverse with immersive devices like head-mounted displays,\\nAI is a pivotal technology working behind the scenes to\\nbuild a creative and beautiful world, thus bringing a seam-\\nless virtual-reality experience to users. AI can facilitate the\\ncontent creation process, for example, some AI modules like\\nGANverse3D introduced by NVIDIA enable developers and\\ncreators to take photos of objects and then make virtual\\nreplicas. Several DL-based methods have been proposed for\\nrendering 3D objects (including human body parts), which\\ncan achieve very impressive accuracy while presenting real-\\ntime processing accelerated by both software (e.g., PyTorch3D\\nlibrary from Facebook AI and TensorRT from NVIDIA) and\\nhardware (e.g., GPUs) Meta just very recently introduced the\\nAI research supercluster (RSC) [19], believed as among the\\nworld-class fastest AI supercomputer that will speed up AI\\nresearch and be served for building the metaverse. Further-\\nmore, RSC can help AI researchers and scientists develop\\nbetter DL models from massive data, including text, speech,\\nimage, video, for various services/applications. Accordingly,\\nany achievements and outcomes derived from RSC will be\\nused as fabrics to build the metaverse platform, in which AI-\\ndriven products will be of considerable importance.\\nIII. AI FOR THE METAVERSE : TECHNICAL ASPECT\\nThis section investigates the state-of-the-art AI-based meth-\\nods in six technical aspects: natural language processing,\\nmachine vision, blockchain, networking, DTs, and neural\\ninterface; which present the potential for the metaverse as\\nshown in Fig. 4. Accordingly, the experience of users in the\\nmetaverse is enhanced signiﬁcantly with nearly no boundary\\nbetween the virtual world and the real world.A. Natural Language Processing\\nNatural language processing (NPL), also known as com-\\nputational linguistics, encompasses a variety of computational\\nmodels and learning processes to solve practical problems of\\nautomatically analyzing and understanding human languages,\\nincluding speech and text. Besides, the ﬁeld of NLS considers\\nmany topics, such as speech-to-text, text-to-speech, conver-\\nsation design, voice branding, and multi-language and multi-\\ncultural in voice. Furthermore, NLP plays a vital role in the\\nmetaverse regarding intelligent virtual assistants (a.k.a., chat-\\nbot). Particularly, NLP is principally responsible for enabling\\nchatbots to understand complicated human conversation in\\nthe context of varying dialects and undertones. Empowered\\nby AI, chatbots can answer nuanced questions and learn\\nfrom interaction to improve the quality of responses. The\\nAI chatbots are developed to assist users in some virtual\\nenvironments like the metaverse.\\nAs one of the most important tasks in NLP, language\\nmodeling predicts words or simple linguistic units by cap-\\nturing syntactic and semantic relations of preceding words\\nand units, which is useful for machine translation and text\\nrecommendation. In [20], many neural networks with key-\\nvalue attention mechanisms were built and evaluated on the\\nWikipedia corpus dataset to conclude that RNNs and LSTM\\nnetworks with the attention mechanisms can outperforms\\nlarge-scale networks while reducing memory in use. In [21],\\na memory network with residual connection was designed to\\nimprove the performance of language modeling in terms of\\ntest perplexity if compared with regular LSTM [22] having\\nan equivalent size. Some recent CNNs have been leveraged\\nto address the long-term dependencies in long sentences and\\nshort paragraphs, especially being efﬁcient to speciﬁc and\\ncomplicated word patterns [23]. Some deep networks were de-\\nsigned with advanced modules and connection structures to en-\\nhance language modeling efﬁciency, such as gated connection\\nand bi-directional structure [24]. Besides word-aware language\\nmodels, many character-aware models have been introduced\\nwith AI algorithms to deal with various diversiﬁed languages\\nin the world. Both the CNN and LSTM architectures [25]\\nwere applied to analyze the representation of words from\\ncharacters as the input. Some models were evaluated on many\\ndatasets of English, German, Spanish, French, and Arabic, in6\\nAugmented Reality\\nNon-environment aware\\n2D/3D content is overlaid\\nonto the physical spaceMixed Reality\\nEnvironment aware\\n2D/3D content is overlaid\\nonto the physical spaceUser is completely\\nimmersed into a virtual\\nworldVirtual RealityExtended Reality\\nEntire experience\\nspectrum from fully\\nvirtual to fully real\\nFig. 5. The difference between AR, MR, and VR under the umbrella of XR.\\nwhich they showed the effectiveness in identifying preﬁxes\\nand sufﬁxes, recognizing hyphenated words, and detecting\\nmisspelled words [26]–[28]. Generally speaking, character-\\naware and word-aware modeling techniques allow natural\\nlanguage understanding systems to extract syntactic and se-\\nmantic information for some common tasks in the metaverse,\\nsuch as part-of-speech tagging, named-entity recognition, and\\nsemantic role labeling.\\nDL has been further exploited to overcome the learning\\nlimitation of conventional ML algorithms and effectively deal\\nwith many challenging tasks in NLP. Some CNNs with sample\\nand advanced architecture in [29] were leveraged to cope\\nwith multiple sentence-based tasks, such as sentiment pre-\\ndiction and question type classiﬁcation. Moreover, sentiment\\nanalysis and recognition tasks may require feature extraction\\nof aspects and sentiment polarities [30], which are potential\\nto improve the reliability and ﬂexibility of virtual assistant\\nunits in the metaverse. Natural language generation is an\\nadvanced functionality of chatbot to generate reasonable task-\\nspeciﬁc conversation-oriented text. Some single RNN/LSTM\\nand mixture LSTM-CNN models were proposed to generate\\nshort text in image captioning and long text in virtual question\\nanswer [31]. Besides supervised learning, unsupervised and\\nreinforcement learning with deep models have been adopted\\nfor some speciﬁc NLP tasks, such as text parsing, seman-\\ntic labeling, context retrieval, language interpretation, and\\ndialogue generation [32]. In the metaverse, NLP techniques\\nshould be combined to fully provide text-based and speech-\\nbased interactive experiences between human users and virtual\\nassistant.\\nB. Machine Vision\\nMachine vision, including computer vision and XR in\\ncooperation, is one of the central technologies to obtain the\\nfoundation of the metaverse. The raw data perceived from\\nvisual environments (via optical display and video player)\\nis captured and processed to infer high-level information,\\nwhich is then shown to users over head-mounted devices\\nand others, such as smart glasses and smartphones. Indeed,\\ncomputer vision allows XR devices to analyze and understand\\nuser activities based on visual-based meaningful information.\\nRepresented as avatars in the virtual worlds, the users canfreely move in 3D maps and interact with virtual objects in\\nthe metaverse.\\n1) Extended Reality: XR is deﬁned as an umbrella term that\\nencapsulates VR, AR, mixed reality (MR), and everything in\\ntheir gaps as shown in Fig. 5. Although some revolutionary\\nexperiences are offered for VR and AR, the same original\\ntechnologies are fueling the innovation and development of\\nMR. While AR provides the experiences of graphics, video\\nstreams, and holograms in the physical world and VR offers\\nviewing experiences in a fully immersive digital world, MR\\ncan deliver a transition experience between AR and VR. Along\\nwith these reality technologies, human users can experience\\nthe metaverse and enjoy diversiﬁed services in both the\\nphysical and digital worlds. While XR and AI are distinct\\nsectors, they can be combined to reach a fully immersive in\\nthe metaverse.\\nWhile conventional two-dimensional (2D) videos are lim-\\nited by the small ﬁeld of view (FoV), the 360-degree videos\\nproviding unlimited view-point with all directions are suitable\\nfor VR performance. Many commercial VR headsets are\\ndesigned to satisfy the high-class using requirements, such as\\nperformance and comfortableness, which encompass multiple\\ntasks driven by AI automatically. With the VR headset, a\\nhuman user can experience various services and applications in\\nthe metaverse, and further create hyperreal media contents in\\nthe virtual world. Some AI algorithms have been applied in VR\\ndevices to improve the human-machine interaction experience\\nbased on visual-based information. For the prediction of\\nuser’s eye ﬁxations in some gaze-based applications, such as\\ncontent design and rendering, a DL framework with multiple\\nCNNs [33] was built to deal with various kinds of input\\ndata, e.g., VR image, gaze data, and head data. This model\\neffectively exploited the correlations between eye ﬁxations and\\nother factors likes VR content and headset motion. In [34],\\nneural networks were adopted for human identiﬁcation and\\nauthentication by analyzing periodic behaviors between users\\nand VR gears (e.g., controllers and head-mounted display).\\nThis work’s effectiveness in delivering useful information\\nand treatment recommendation was veriﬁed in collaboration\\nand gaming scenarios and has shown some applicable po-\\ntentials in other scenarios, such as working and shopping.\\nTo enhance users’ quality of experience (QoE) in the virtual\\nworld, an innovative human-machine interface approach [35]7\\nwas proposed by incorporating triboelectric sensory gloves\\nand display components in VR devices to recognize multi-\\ndimensional motion of gestures. As a result, virtual objects\\nwhich are recognized by leveraging ML/DL algorithms can\\nbe manipulated in the real-time VR/AR space. To access the\\ncontents in the metaverse and interact with virtual objects in\\nthe digital world, not only AR headsets but also other devices\\n(e.g., triboelectric gloves, hand-held touchscreen devices, and\\ntabletops) [36] are taken into consideration regarding speciﬁc\\napplications, services, and infrastructures.\\nTo satisfy the service’s demands about high-resolution\\nvideo viewing experiences with VR devices, it is necessary\\nto develop an effective video quality assessment method,\\nin which DL has represented as a powerful tool to obtain\\nquantitative and qualitative benchmark objectives. In [37],\\na high-performance method was developed for VR quality\\nassessment by building a 3D CNN architecture, in which\\nthe video prediction results are validated via some common\\nimage quality assessment metrics without video reference.\\nCompared with the baseline, which performed handcrafted\\nfeature extraction and ML algorithms, the 3D CNN-based\\napproach showed the superiority in term of VR video qual-\\nity assessment and benchmark [38]. Quality assessment was\\nextended for 2D and 3D foveated-compressed videos in [39],\\nwhich allows VR systems to effectively handle the limited\\ndata transmission bandwidth. The advantages and limitations\\nof current video quality assessment methods were analyzed\\nand exposed in [40], which can be useful to design an effective\\nvideo transmission mechanism for various AR systems and\\ndiversiﬁed video contents. On the way to become the next\\nmainstream for consumers and business, MR is deﬁned as a\\nblend of physical and digital worlds, which establishes the\\nnatural and intuitive interactions between 3D human, com-\\nputer, and surrounding environment [41]. This new reality is\\nactivated by the recent revolutions in computer vision, graphic\\nprocessing, display, remote sensing, and AI technologies [42].\\nCompared with VR and AR, MR has more potentials for\\nthe metaverse thanks to its hybrid physical-virtual experiences\\nvia two main types of devices2: holographic device with see-\\nthrough display allows users to manipulate physical objects\\nwhile wearing it and immersive device allows users to interact\\nwith virtual objects in the virtual world. In the future, new\\ndevices to enhance the visual-interactive experiences of users\\nin the metaverse should minimize the different gaps in terms\\nof speciﬁcation and utility between holographic devices and\\nimmersive devices.\\n2) Computer vision: In the last decades, computer vision\\nhas been empowered by AI, especially DL with a variety\\nof network architectures to improve the overall accuracy of\\nvisual systems with efﬁcient cost thanks to high-performance\\ngraphic processing units. Some fundamental computer vision\\ntechnologies are potential to enhance the experience of human\\nusers in the metaverse, thus enabling users in the physical\\nworld to interact with the virtual environment in the digital\\nworld smoothly.\\n2https://docs.microsoft.com/en-us/windows/mixed-reality/discover/mixed-\\nrealitySemantic segmentation and object detection are two funda-\\nmental tasks in the computer vision domain, where semantic\\nsegmentation categorizes each pixel in an image to be one of\\npre-deﬁned semantic classes [43] and object detection aims to\\nlocalize all possible objects in an input image by drawing\\ncorresponding bounding boxes with object information in\\ntag [44]. Early segmentation works mostly adopted local fea-\\nture extraction and tracking in cooperation with classiﬁcation\\nalgorithms, which were limited by unacceptable segmenta-\\ntion performance when dealing with large-messy datasets.\\nRecently, numerous DL-based approaches have shown con-\\nsiderable improvement in terms of performance by exploiting\\ndifferent deep architectures if compared with traditional meth-\\nods [45]. The powerful capability of CNNs in extracting deep\\nvisual features at multi-scale image resolutions has been ex-\\nploited to design advanced segmentation models in [46]–[50].\\nFor example, DeepLab [48] leveraged atrous convolution to\\nimprove feature learning efﬁciency by enlarging the receptive\\nﬁeld of ﬁlters while keeping a small number of parameters or\\na low computational cost. Because of learning classiﬁcation\\nmodels at pixel level, image segmentation usually consumes\\nconsiderable computation and large memory [51], [52]. To\\novercome this challenge, some efforts have been presented\\nwith some skills of network designs and learning techniques,\\nsuch as transfer learning [53]. The virtual environment in the\\nmetaverse is usually built with diversiﬁed visual units (e.g.,\\nsingle objects and multi-object modules); therefore, AI-based\\nobject detection must deal with a huge number of complicated\\nclasses, including real and virtual objects. Numerous recent\\nobject detection works have exploited CNN architectures to\\nachieve impressive performance in terms of accuracy and\\nprocessing speed [54]–[56]. DL-based semi-supervised and\\nunsupervised learning models have been recommended to\\ntackle unseen classes in the training dataset [57]. Some natural\\nproblems of object detection in the 3D environment, such as\\nocclusion, illumination variation, and view-point change, have\\nbeen taken into consideration by incorporating advanced image\\nprocessing and depth sensing algorithms [58], [59]. In this\\ncontext, depth estimation can improve the accuracy of object\\npositioning [60] in the 3D virtual world, but more geometric\\nsensors are required to estimate depth information.\\nIn the virtual world, some image quality reduction prob-\\nlems, such as noise, blurring, and low resolution, should be\\naddressed to enrich the visual perception of users. In the\\nperspective of image processing and computer vision, these\\nproblems are studied over two tasks: image restoration and\\nimage enhancement. In [61], a decomposition-guided multi-\\nscale CNN-based method was proposed to remove single\\nimage haze, in which deep residual structure and U-Net [62]\\nlearning frame are combined to improve decomposed image\\ncomponents (so-called as feature maps) while avoiding color\\ndistortion. Some other advanced image restoration works\\nexploited CNN architectures to reduce image compression\\nartifacts, restore clean images from downscaled and blurred\\nimages, and reconstruct missing details [63]–[65]. It is noted\\nthat the differences in terms of image quality and video spec-\\niﬁcation between the clean virtual contents and real displayed\\nimages/videos can appear in VR devices [66], [67]. These gaps8\\nhuman, standing\\nflowers, pink, yellow, orangeflowers, pinkScene: garden\\nflowers,\\npink, violet\\xa0human, watering\\xa0\\nsprinkler, greenfence lamp, white\\nFig. 6. Computer vision in the metaverse with scene understanding, object\\ndetection, and human action/activity recognition.\\ncan be ﬁlled effectively with AI-empowered image restora-\\ntion methods, such as blur estimation, hazy removal, color\\ncorrection, and texture reconstruction, but the computational\\ncomplexity should satisfy the real-time video processing speed\\n(usually measured by frames per second – FPS metric) [68]\\nto guarantee high-class user experience in the metaverse.\\nImage enhancement has been widely considered for XR with\\nsome common tasks, such as contrast increment and super-\\nresolution construction. In the past, many traditional image\\nenhancement methods have been studied by applying image\\nprocessing techniques, for example, histogram analysis and\\nimage decomposition [69], [70]. Recently, numerous impres-\\nsive works of image enhancement have achieved considerable\\nperformance improvement by exploiting ML algorithms [71],\\nespecially DL with CNN architectures [72]–[75]. For example,\\na convolutional down-sampling and up-sampling network was\\nintroduced in [72] to improve the overall contrast of images,\\nin which the deep features of RGB (red, green, and blue)\\nchannels are combined via a feature-based fusion scheme to\\nobtain cross-channel contrast balance. In [76], a fully CNN\\nfor image super-resolution was proposed with a lightweight\\nstructure, which can learn an end-to-end relation between\\ninput low-resolution images and output high-resolution im-\\nages. Compared with some traditional sparse-coding-based\\nmethods [77], this approach has shown the superiority in terms\\nof image quality and processing speed. Super-resolution can\\nbecome a cost-efﬁcient solution that allows service providers\\nto build high-resolution virtual worlds from low-resolution\\nimage/video sources.\\nIn the metaverse, play users can control their avatars (or\\nvirtual characters) and interact with other users or non-\\nplayer characters (NPCs), in which the posture and action\\nof avatars should be estimated and recognized automatically\\nwith the support of motion sensing interactive devices, such\\nas controllers, gloves, and cameras [78]. While human pose\\nestimation aims to identify the body parts (or key body joints\\nof a skeleton) and then track them in the real-time environ-\\nment [79], action recognition allows systems to understandsingle actions and complex interactive activities (e.g., human-\\nmachine interaction and human-human interaction) [80]. To\\ndeal with the problem of estimating human pose in cluttered\\nenvironments, two discriminative models based on standard\\nstructural support vector regression (SVR) and latent struc-\\ntural SVR were studied in [81], which are capable of ex-\\ntracting structural dependencies as the correlations between\\nlocal features regarding pose representation. To improve the\\naccuracy of body part localization and deal with varying views,\\nthe depth information acquired by depth cameras has been\\nlearned along with color information by advanced ML and DL\\nmodels [82]–[84]. In these works, some CNN architectures\\nwith high-class structural connections, such as dense layer\\nconnection, skip connection, and channel-attention connection,\\nwere designed to estimate skeletal joints precisely besides\\naddressing some challenging problems in computer vision like\\nobject occlusion. In the line, human pose estimation has a\\nclose relation to action recognition, where the captured body\\ninformation is useful to identify actions via pattern recognition\\nmodels. Instead of detecting instant posture that can expose\\nhigh confusion, many current works have tracked body motion\\nin the temporal domain for a long-term observation to improve\\nthe accuracy of action recognition. For example, some gener-\\native statistic models have been developed in [85]–[87] to an-\\nalyze human pose transition by capturing the spatio-temporal\\ngeometric features between different body parts. Notably, the\\nlast decade has witnessed the revolution of visual-based ac-\\ntion recognition with DL to signiﬁcantly improve recognition\\naccuracy and effectively deal with numerous realistic single\\nactions and grouped activities [88]. Some methods proposed\\ninnovative networks with advanced CNN architectures [89]\\nand hybrid CNN-RNN architectures [88] to improve learning\\nefﬁciency of action discrimination models. Additionally, hand\\ngesture recognition, gait identiﬁcation, and eye tracking [90],\\n[91] have been considered to improve interactive experiences\\nin XR environments.\\nC. Blockchain\\nIn general, blockchain is deﬁned as a digital ledger that\\ncontains a list of recorded transactions and tracked assets\\ninterconnected in a business network by using cryptography\\ntechniques. Blockchain can provide immediate, shared, and\\ntransparent information stored in an immutable and impen-\\netrable ledger which can be accessed by only the network\\nmembers with permission [92]. A typical blockchain network\\ncan track orders, payments, accounts, and other transactions.\\nIn the metaverse, a large amount of data (e.g., videos and\\nother digital contents) is acquired by VR devices, transmitted\\nover networks, and stored in data center without any security\\nand privacy protection mechanisms, which can become the\\nsensitive target of cyberattacks. In this context, blockchain\\nwith several unique features reveals a promising solution for\\nsecurity and privacy issues in the metaverse [93], especially\\nwhen it is empowered by AI technologies. Besides, many\\ncreative activities and events offered by service providers to\\nusers will yield numerous in-metaverse objects/items (a.k.a.,\\ndigital assets) which should be recorded and tracked via\\ntransparent transactions with smart contracts in blockchain.9\\nSmart\\nWatchSmart\\nTVSmart\\nCameraSmart\\nPhoneSmart\\nSensorServer Smart Contract Chain Network\\nSmart\\nCitySmart\\nHomeUtility\\xa0 ControlMarket\\nPlaceUser 1\\n User 2\\n User k\\n User NAccess\\nLayerBlockchain\\nLayerApps\\nLayerIoT System\\nLayerReal Time\\nAnalyticsData\\nAnalytics\\nData Acquisition Layer\\nPre-processing\\nEvaluation Phase\\nMachine\\nLearning ModelCloud\\nPlatform\\n- Trained Model for Access Layer- Import Trained Model to Cloud1\\n21\\n2\\nFig. 7. A blockchain-based IoT framework with ML to enhance security and\\nprivacy.\\nIn the last decade, numerous advanced methods for data\\nacquisition, storage, and sharing have been proposed by com-\\nbining blockchain and AI technologies in various application\\ndomains to obtain high data security and privacy [94], which\\nhave shown great potential to be deployed in the metaverse.\\nIn [95], various conventional ML algorithms (e.g., clustering,\\nSVM, and bagging) and innovative DL architectures (e.g.,\\nCNN and LSTM) were investigated for data analytics to\\ndetect and classify cyberattacks in blockchain-based networks.\\nSome other concerns were also considered in this work, such\\nas incentive mechanisms to encourage users to contribute\\nauthenticated data, AI-based smart contract evaluation, and\\ncost-efﬁcient model learning in on-chain environment. For\\nthe Internet of Things (IoT)-aided smart cities, an effective\\nprivacy-preserving and secure framework [96] was introduced\\nby integrating blockchain with enhanced proof of work and\\nML with data transformation, which in turn robustly deals\\nwith various cyberattacks in smart city networks. In [97],\\ndeep extreme learning machine was exploited in a resource-\\nefﬁcient blockchain-based IoT framework, which improved the\\nsystem security and privacy based on data interpretation and\\nabnormality prediction. This framework (see Fig. 7) has shown\\nhigh performance of fraud detection and threat prediction, and\\ncan be extended for dealing with security and privacy problems\\nin data storage and sharing instead of data collection. Recently,\\nDL has replaced traditional ML in terms of cooperating with\\nblockchain to solve some challenging security and privacy\\nissues of big data, where ﬁve essential characteristics of\\nbig data (i.e., velocity, volume, value, variety, and veracity)\\nare presented. For example, DeepChain [98], a CNN-based\\nblockchain framework, was developed to ensure the privacy\\nand integrity of data contributed by participants in a network.\\nDeep RL was exploited to achieve a secure mobile ofﬂoading\\nin multi-access edge computing (MEC) based blockchain\\nnetworks [99] and to obtain a secure vehicular crowdsensing in\\nthe blockchain-based Internet of vehicles (IoV) systems [100].\\nFederated learning (FL) has recently emerged as an effective\\nsolution to address the privacy problems of data sharing,\\nin which multiple users train AI models with their ownlocal data and collaboratively learn a global model at the\\nserver by a parameter aggregation mechanism. In [101],\\nFL was applied to address privacy issues in data sharing\\namong multiple untrusted parties in a blockchain network.\\nThis work integrates FL into a proof of training quality, a\\nnovel consensus mechanism, of permissioned blockchain to\\nreduce computing and communication costs. To guarantee\\nhigh privacy of massive data generated by heterogeneous IoT\\ndevices, FL was deployed in a blockchain-based resource\\ntrading system [102]. A smart contract-based incentive algo-\\nrithm was proposed to encourage edge nodes to contribute and\\nevaluate FL tasks. For vehicular edge computing in intelligent\\ntransportation systems, FL was combined with blockchain to\\ncollaboratively detect malicious attacks [103]. While FL can\\nofﬂoad the trained intrusion detection model to distributed\\nedge devices to reduce computing resources of the central\\nserver, blockchain can ensure the security of the aggregation\\nmodel in both the model storage and sharing processes.\\nBesides data security and privacy, interoperability is another\\nimportant concern in blockchain to collaborate with different\\nparties using different data infrastructures. For example, a\\nlearning analytics framework [104] was studied to obtain\\nsolid interoperability between multiple blockchain participants\\nwhich have to share a single ledger. Integrating blockchain\\ninto FL were recently found in computing resource allocation\\nand management applications [105], [106] to address various\\nproblematic issues in centralized systems, such as external\\ncyberattacks, server malfunctions, and untrustworthy server.\\nIn the metaverse, wherein multiple parties join and contribute\\ndigital content having different formats and structures, data\\nsecurity, privacy, and interoperability can be fully handled by\\ncollaboratively developing blockchain and AI.\\nD. Networking\\nThe metaverse serves a massive number of users regarding\\npervasive network access over wireless networks. In the last\\ndecade, several innovative technologies have been introduced\\nto improve the overall performance of wireless communication\\nand networking systems, in which AI has been intensively\\nused at multiple layers of a network architecture [107]. Real-\\ntime multimedia services and applications in the metaverse\\nusually demand a reliable connection with high throughput\\nand low latency to guarantee a basic-level user experience\\nat least. As the requirements of ﬁfth-generation (5G) net-\\nworks, the peak data rate should be around 10 Gbps (gi-\\ngabits per second) and the end-to-end delay cannot exceed\\n10 ms (millisecond). In this context, ultra-reliable and low-\\nlatency communications (uRLLC) represent the foundation\\nto enable the development of emerging mission-critical ap-\\nplications [108]. Several optimization algorithms have been\\nintroduced to achieve uRLLC in 5G networks and beyond\\n(e.g., sixth-generation 6G), but most of them require high\\ncomputing resources. ML and DL have shown great poten-\\ntial to effectively handle existing challenging tasks, such as\\nintelligent radio resource allocation [109], in 5G/6G networks\\nwhile meeting a very low latency. RL was leveraged to address\\nthe resource slicing problem for enhanced mobile broadband10\\n(eMBB) and uRLLC [110], in which the complicated patterns\\nof resource allocation and scheduling are formulated to col-\\nlaboratively learn network states and channel conditions. In\\nanother work [111], RL showed the effectiveness in terms\\nof joint subcarrier-power management and allocation, thus\\nsigniﬁcantly reducing latency and improving reliability on the\\nInternet of controllable things. Particularly, this work proposed\\na double Q-learning network to optimize the total spectrum\\nefﬁciency via subcarrier assignment and power control policy,\\nand accelerate learning convergence. As a vital role to enable\\nuRLLC, efﬁcient radio resource management was investigated\\nin [112] with a distributed risk-aware ML approach to monitor\\nand manage the transmission of non-scheduled and scheduled\\nuRLLC trafﬁcs.\\nLately, DL has been exploited for many tasks in uRLLC,\\nincluding spectrum management, channel prediction, trafﬁc\\nestimation, and mobility prediction. Two advanced CNN ar-\\nchitectures, namely MCNet [113] and SCGNet [114], were\\ndesigned in physical layer to automatically identify the mod-\\nulation types of incoming signals, which in turn allows the\\nreceiver to demodulate accurately and enhance the spectrum\\nutilization efﬁciency accordingly. To overcome the high com-\\nputational cost of conventional channel state information (CSI)\\nestimation approach, an online CSI prediction method [115]\\nwas proposed a supervised learning framework by combining\\nCNN and LSTM, in which two-stage training mechanism was\\ndeployed to improve the robustness and stableness of CSI es-\\ntimation in practical 5G wireless systems. In [116], an end-to-\\nend CNN architecture was designed with 3D convolution for\\nintelligent cellular trafﬁc forecasting, in which the deep model\\ncan learn the underlying correlations of trafﬁc data in both the\\nshort-term and long-term spatial patterns. Besides achieving\\nhigh accuracy of trafﬁc prediction, the deep network showed\\nthe effectiveness with different real-world scenarios, such as\\ntrafﬁc congestion data and crowd ﬂow data. In conclusion,\\nwith ML algorithms and DL architectures, AI is a powerful\\ntool to address many challenging problems of uRLLC in future\\nwireless networks, which allows users to experience high-class\\nintegrated services in the metaverse with the guarantee of high\\nthroughput low latency.\\nE. Digital Twins\\nAs a digital representation of real-world entities, a DT can\\nsynchronize operational assets, processes, and systems with\\nthe real world along with some other regular actions, such\\nas monitoring, visualizing, analyzing, and predicting [117].\\nDTs are at the central of where the physical world and the\\nvirtual world interact via IoT connections [118]; and therefore,\\nany change in the real world will be rejected in the digital\\nrepresentation. With these distinctive properties, DT is found\\nas one the fundamental building sectors of the metaverse and\\nplays as the gateway for users to enter and enjoy services\\nin the virtual world by creating exact replications of reality,\\nincluding structure and functionality. For example, technicians\\ncan maneuver 3D representations of complex systems at multi-\\nlevel sophistications (i.e., descriptive, informative, predictive,\\ncomprehensive, and autonomous) for a wide spectrum of\\nDevices with processing capability\\nfor users to experience in the\\nmetaverseBase station (BS) with caches\\nand edge servers\\xa0\\xa0Centralized control planeCentral server\\nUser-level tasks\\n\\xa0 Predict traffic\\n\\xa0 Estimate mobility\\n\\xa0 Detect anomalyBS-level tasks\\n\\xa0 Control user access\\n\\xa0 Design scheduler\\n\\xa0 Optimize resourceNetwork-level tasks\\n\\xa0 Routing\\n\\xa0 Handover\\n\\xa0 Network slicing\\nML algorithms /\\nDL architectures\\nLearning models for\\ndetection, classification,\\nand regressionFig. 8. A general architecture in 5G and beyond for metaverse services and\\napplications, in which AI with ML algorithms and DL models contribute in\\nmulti-level tasks.\\npurposes, such as technical training and commercial cus-\\ntomization. Accordingly, DTs allow application developers\\nand service providers to reconstruct virtual replications of\\nmachines and processes, in which any kind of physical analysis\\ncan be done remotely with AI [119].\\nFor industry 4.0, a reliable DT framework [120] was pro-\\nposed for sensor-fault detection, isolation, and accommoda-\\ntion, in which a multi-purpose ML method with multi-layer\\nperception neural network was deployed to validate sensory\\ndata, estimate fault condition, and identify fault sensor. As\\na digital replication to operate human-robot welding actions,\\na DT system in [121] was developed along with VR and\\nAI technologies to monitor and analyze welder behaviors. A\\ngeneric ML framework with domain transformation, feature\\nengineering, and classiﬁcation was applied to recognize proper\\nwelding behaviors based on the data acquired from a bi-\\ndirectional ﬂow between robot and VR. In [122], a data-driven-\\nbased DT framework (see Fig. 9) was studied to improve\\nhealth diagnosis performance and promote better health op-\\neration in intelligent healthcare systems. DTs contributed in\\ndifferent phases to create virtual replications of patent health\\nproﬁles, carry out collaborative activities of health profes-\\nsionals, and formulate a universal treatment plan for patients\\nin same cases. ML models were built to learn meaningful\\ninformation from raw data collected by the Internet of Medical\\nThings (IoMT) devices to early detect health abnormalities\\nand precisely recognize health problems. In another work\\nthat proposed a cyber-physical framework for smart urban\\nagriculture services and applications [123], DTs were designed\\nto replicate a virtual representation of farming production,\\nin which the sensory data acquired by practical sensors was\\nprocessed by ML algorithms in a decision support system. To\\nadapt to different types of product magniﬁcation, the DTs were\\nbuilt from small functioning modules to a whole process twin.\\nWith a great capability of automatically learning features\\nfrom high-dimensional unstructured data and effectively deal-11\\nData\\nAnalyticsCleaning Preprocessing\\nMachine\\nlearningRepresentation\\nResult\\nRepositoryRaw Data\\nRepository\\nPhysical TwinHealthcare\\nProfessionalsResult\\nFeedbackResult & Feedback\\nSimilar-case PatientsDigital twinsResult & Feedback\\nRaw\\nDataDigital Twin Architecture\\nStorageSmart System\\nFeedbackMonitoring\\nIoMT\\nDevices\\nFig. 9. A data-driven DT framework for intelligent healthcare systems using\\nML to process raw data of IoMT devices.\\ning with spatiotemporal learning models, DL has been recently\\napplied in DT architectures for different services and appli-\\ncations. In [124], a DT architecture was developed for the\\nedge computing-aided Internet of vehicles (IoV) to improve\\nthe utilization efﬁciency of vehicle’s computational resources.\\nTo overcome the overload problem of edge devices, deep Q-\\nnetwork optimized the function approximation of DL and RL.\\nFor the performance investigation of uRLLC services and\\ndelay tolerant services in mobile edge computing systems,\\na DT framework was built in [125] by replicating a virtual\\npattern of the real network environment. Remarkably, DL\\nwith feedforward neural network architecture was carried\\nout to deal with varying network parameters of real-world\\nphenomena. For industrial IoT, the work in [126] built DTs to\\nsimulate and capture the operation state and real-time behavior\\nof industrial devices, which were map into a digital world. To\\naddress the bias between real entity and its digital replication,\\na trusted-based aggregation with FL was carried out with a\\ndeep RL model to general improve performance while meeting\\nresource constraints. With AI as a powerful analytics tool,\\nDT can improve system performance, reduce process-related\\nincidents, minimize maintenance costs, and optimize business\\nand production. In addition, DT allows users to view the\\nmetaverse as an advancing replication of reality with full real-\\ntime synchronization from the physical world.\\nF . Neural Interface\\nTechnology is deﬁnitely enriching the world around us by\\nenhancing the human experience and fully ﬁlling gap between\\nreality and virtual world in the metaverse. In this context, the\\nmost immersive popular interface to interactive with the virtual\\nwork is a VR headset with a controller. Many technology com-\\npanies currently pay attention to neural interfaces, so-called\\nbrain-machine interfaces (BMIs) or brain-computer interfaces\\n(BCI), that go beyond VR devices. The BMIs help to nearly\\nclean the borderline between human and wearable devices.\\nMany BMIs detect neural signals using external electrodes or\\noptical sensors that adhere to the skull and other parts of the\\nNeural activity\\ngeneration\\nData\\nacquisition &\\nstimulation\\nData\\npreprocessingPattern\\nrecognitionApplications\\n& services\\nBrain signals\\nRaw data\\nFine dataAcquisition\\nactionUser\\nfeedback\\nStimulation\\nactionFiring\\npatternStimulation\\nStimulation\\nmodelNeural data\\nacquisiton flow\\nNeural\\nstimulation flowFig. 10. A common BMI cycle with primary components for processing\\nneural signals and responding neural stimulations.\\nhuman body. According to these noninvasive devices, which\\nonly read and control mind at a rudimentary level, BMIs\\ncan manipulate thoughts with transcranial electromagnetic\\npulses. Fig. 10 describes a common BMI cycle with primary\\ncomponents for processing neural signals and responding\\nneural stimulations [127]. Besides data engineering techniques\\nin the preprocessing stage, AI/ML algorithms in the pattern\\nrecognition stage enable analysis of complicated and sensitive\\nneural signals accurately.\\nWith electroencephalogram (EEG) signal as one of the most\\npopular inputs of BCI systems, the work in [128] studied a\\nbrain signal classiﬁcation by two learning approaches: one\\nis ofﬂine unsupervised classiﬁcation and another is simulated\\nonline supervised classiﬁcation. Besides that, two approaches\\nachieved lower computational cost and better performance\\nin common tasks, e.g., motor imagery, mental analysis, and\\nevent-related potential, the ofﬂine unsupervised mechanism\\ndid not require signal labeling for new subjects in the learning\\nphase. As building an accurate predictive model in BCIs to\\ndecipher brain activities into communication and control com-\\nmands, the work in [129] learned discriminative spatiotempo-\\nral features to seize the most relevant correlations between\\ndifferent neural activities from EEG signals. Based on the re-\\nconstructed signal waveforms containing dominant frequency\\ncharacteristics as feature vectors with lower dimensionality,\\nseveral ML algorithms, e.g., logistic regression (LR), Naive\\nBayes, and SVM, were applied to investigate the performance\\nof ERP. In [130], the feasibility of using visual hemisphere\\nto extract relevant information about the spatial location of\\ntargets in aerial images was investigated with feature selection\\nand SVM classiﬁcation, which were deployed in a rapid\\nserial visual presentation (RSVP) procedure. Concretely, by\\nlearning ERP patterns from extracted discriminative features\\nof EEG signals, the concerning target and its location in aerial\\nimages can be identiﬁed. As the effort to increase the correct\\nclassiﬁcation rate of EEG signals in BCIs, an advanced ML\\nframework was introduced in [131] by combining an improved\\ncommon spatial pattern algorithm and a transfer learning12\\nmechanism. Besides achieving high accuracy of classifying\\nleft-hand and right-hand imaginary movements, the trained AI\\nmodel can be used for other classiﬁcation and recognition tasks\\nin the same domain via knowledge transferring technique.\\nBased on the superiority of capsule network (CapsNet)\\ncompared with traditional neural networks in terms of feature\\nextraction and feature explanation, the work in [132] applied\\nCapsNet to improve the accuracy of ERP detection in BCI\\nsystems. With highly discriminative spatial features and key\\ntemporal correlations extracted from EEG signals by capsule\\nlayers, CapsNet not only outperformed some state-of-the-\\nart learning models (e.g., linear discriminant analysis and\\nCNN [133]) but also obtained the practicality with differ-\\nent common spellers in the cognitive neuronscience domain.\\nIn [134], a hybrid DL framework was proposed with multi-\\ndirectional CNN and bidirectional LSTM in an accurate brain-\\ncontrolled robotic arm system. This proposed learning ap-\\nproach effectively calculated underlying spatial signal corre-\\nlations in time, boosting the decoding performance for 3D\\nmulti-directional arm-based object grasping tasks. Inspired by\\nGoogleNet [135], the work in [136] proposed EEG-Inception,\\na novel CNN for EEG-based classiﬁcation tasks in BCI sys-\\ntems, which involved multiple inception modules in improving\\nfeature learning efﬁciency. Furthermore, an effective training\\nstrategy that incorporated cross-subject transfer learning and\\nﬁne-tuning to reduce calibration time of ERPs and demonstrate\\nthe feasibility in real-world assistive applications. In the future,\\nbrain-computer interfaces will truly promote the ultimate im-\\nmersive interaction between the reality and the virtual world in\\nthe metaverse via consumer-ready mind-control systems. The\\nexisting AI-enabled works on the subject of six concerning\\ntechnical aspects, which are promisingly for the metaverse,\\nare summarized in Table I.\\nIV. AI FOR THE METAVERSE : APPLICATION ASPECT\\nThis section conveys the existing AI-aided works in the\\nperspectives of four key applications: healthcare, manufactur-\\ning, smart cities, and gaming (see Fig. 11); which are proba-\\nbly considered to delivery specialized services in the meta-\\nverse Besides, some other potential applications, including\\nE-commerce, human resources, real estate, and decentralized\\nﬁnance, are discussed in brief.\\nA. Healthcare\\nThe health industry has recently started exploiting some\\nrevolutionary techniques like VR and big data incorporated\\nwith AI in software and hardware to increase the proﬁciency\\nof medical devices, reduce the cost of health services, improve\\nhealthcare operations, and expand the reach of medical care.\\nFrom a 2D environment to a 3D virtual world, the metaverse\\nallows users to learn, understand, and share patients’ health\\nconditions and medical reports in an immersive manner. By\\nmeans of VR/XR systems, AI plays a vital role in many\\nhealthcare and medical sectors, for example, achieving better\\nefﬁciency in providing diagnosis, delivering accurate and faster\\nmedical decisions, providing better real-time medical imagingand radiology, and supporting more convenient simulated\\nenvironments to educate interns and medical students.\\nIn many wearable devices for healthcare and wellness\\napplications and services [137], AI has been applied to au-\\ntomatically recognize complex patterns of sensory data. For\\nsupporting physicians and health-wellness experts to make\\ndecisions in daily living assistance and early healthy risk\\nawareness, a physical activity recognition method was intro-\\nduced in [138] by using the sensory data of multiple wear-\\nable devices. The method combined the globally handcrafted\\nfeatures and locally deep features (i.e., extracted by a deep\\nCNN) over an intermediate fusion mechanism to improve the\\nactivity recognition rate. In [139], a novel encoding algorithm,\\nnamely Iss2Image, was introduced to convert inertial sensory\\nsignal (e.g., accelerometer, gyroscope, and magnetometer) into\\na color image for CNN-based human activity classiﬁcation.\\nFurthermore, a lightweight CNN with few layers in a cascade\\nconnection was designed to learn the physical activity patterns\\nfrom encoded activity images. In [140], a system of fall\\ndetection using wearable devices was proposed for IoT-based\\nhealthcare services, in which a hierarchical DL framework\\nwith CNN architectures was developed for collaboratively\\nprocessing sensory data at local devices and a cloud server. As\\ncapably working with multiple wearable devices (e.g., smart-\\nphone, smartwatch, and smart insoles), the system yielded high\\ncorrect detection rate with high data privacy. Besides CNN,\\nRNN and LSTM networks were exploited to process wearable\\nsensory data in some early healthy risk attentions, such as fall\\ndetection and heart failure [141].\\nWith the great success of DL, especially CNN architectures,\\nin the image processing and computer vision domains, few\\nrecent years have witnessed a vast emergence of DL to\\naddress various challenging tasks of medical image analysis\\nbecause of the requirement of much more specialized knowl-\\nedge from technicians and medical experts if compared with\\nnatural image analysis [142], [143]. For lesion segmentation\\nin breast ultrasound (BUS) images, the work [144] studied an\\nadvanced network, namely saliency-guided morphology-aware\\nU-Net (SMU-Net), by involving an additional middle feature\\nlearning stream and an auxiliary network. The coarse-to-ﬁne\\nrepresentative features from the auxiliary network were fused\\nwith other features (e.g., background-assisted, shape-aware,\\nedge-aware, and position-aware) to effectively discriminate\\nmorphological texture in BUS images. In [145], a cost-efﬁcient\\nunsupervised DL approach was introduced to accelerate the\\nprocessing speed of non-rigid motion estimation of heart in\\nfree-breathing 3D coronary magnetic resonance angiography\\nimages. Replying on a deep encoder-decoder architecture, the\\nnetwork can learn image similarity and motion smoothness\\nwithout ground truth information in a patch-wise manner to\\nsave computing resources signiﬁcantly instead of a regular\\nvolume-wise manner. To overcome the obstacle of increasing\\nnetwork size and computation of 3D CNNs in mining com-\\nplicated patterns in 3D images [144], 2D neuroevolutionary\\nnetworks were investigated for 3D medical image segmenta-\\ntion [146], in which an optimal evolutionary 3D CNN was\\nrenovated to reduce computational cost without sacriﬁcing\\naccuracy. With AI in use as the core technology for data ana-13\\nTABLE I\\nSUMMARY OF AI F ORTHEMETAVERSE IN THETECHNICAL ASPECT .\\nTechnical Aspect Ref Task AI Technique\\nNLP [20]Word and linguistic prediction for language\\nmodeling.RNNs and LSTM networks with the attention mechanisms.\\n[21] Advanced memory network with residual connection.\\n[24] Deep networks with gated connection and bi-directional structure.\\n[25] Analyzing and understand the representation of\\nwords from charactersGeneral deep networks with CNN and LSTM architectures.\\n[27] Identifying preﬁxes and sufﬁxes and detecting mis-\\nspelled wordsDL framework with CNN, Bi-LSTM, and conditional random ﬁeld.\\n[29] Sentiment prediction and question type classiﬁca-\\ntion.Various CNNs and LSTM networks with simple structures and\\nadvanced-designed architectures.\\n[31] Generate short text in image captioning and long\\ntext in virtual question answer.DL framework with single RNN/LSTM and mixture LSTM-CNN\\nmodels.\\n[32] Semantic labeling, context retrieval, and language\\ninterpretation.Unsupervised and reinforcement learning with common RNN/LSTM\\nand CNN models.\\nVision Machine [33] Forecasting eye ﬁxations in VR task-oriented vir-\\ntual environment.A DL framework with multiple CNNs for feature extraction and\\nprediction.\\n[37] VR quality assessment for 2D and 3D\\nfoveated-compressed videosDL framework with CNNs architecture, in which 3D convolutional\\nlayers are built in feature extraction blocks. [39]\\n[33] Forecasting eye ﬁxations in VR task-oriented vir-\\ntual environment.DL framework with multiple CNNs for feature extraction and pre-\\ndiction.\\n[48]Semantic segmentation and object detection.CNNs with atrous convolution.\\n[51] CNNs with channel-wisely and spatially attentional schemes.\\n[56] CNNs with pixel-wise local and global attention pooling-convolution.\\n[57] CNNs with knowledge transfer via visual similarity and semantic\\nrelatedness.\\n[59] CNNs with 3D object-object relation graphs.\\n[61] Image/video quality enhancement (e.g., hazy\\nremoval, color correction, texture reconstruction,\\nand super-resolution)Decomposition-guided multi-scale CNN architectures with deep\\nresidual structure and U-Net learning frame.\\n[72] Combination of deep features extracted by CNNs via a feature-based\\nfusion scheme.\\n[73] Full 3D CNN architectures with simultaneous and separated spatial-\\nspectral joint feature learning mechanisms.\\n[81] Human pose estiation and action/activity\\nrecognition.Discriminative model with latent structural SVRs.\\n[82] CNNs with dense layer connection and channel-attention connection.\\n[87] Generative models with latent Dirichlet and Pachinko allocations.\\n[89] Advanced CNNs with geometric feature transformation.\\nBlockchain [95] Detection and classiﬁcation of cyberattacks in\\nblockchain-based networks.Conventional ML algorithms (e.g., clustering, SVM, and bagging)\\nand DL architectures (e.g., CNN and LSTM).\\n[97] Resource management in blockchain-based IoT\\nframework.Deep extreme learning machine.\\n[102] Preservation of data privacy of heterogeneous IoT\\ndevices.FL framework with deep model using average aggregation mecha-\\nnism.\\n[103] Detection of malicious attacks in intelligent trans-\\nportation systems.FL framework with CNN model averaging and training.\\nNetworking [109] Resource slicing problem for eMBB and uRLLC. RL with a policy gradient based actor-critic learning mechanism.\\n[111] Subcarrier-power management and allocation. RL with double Q-learning network.\\n[112] Management of the transmission of non-scheduled\\nand schedule uRLLC trafﬁcs.Supervised learning framework with conventional ML algorithms.\\n[113] Automatic modulation classiﬁcation in wireless\\nsystems.Advanced CNN architecture with sophisticated-designed modules of\\nconvolutional layers.\\n[115] Prediction of CSI in 5G wireless systems. A supervised learning framework by combining CNN and LSTM\\nwith two-stage training mechanism\\n[116] Forecasting intelligent cellular trafﬁc. An end-to-end CNN architecture was designed with 3D convolution.\\nDigital Twin [120] Sensor-fault detection, isolation, and accommoda-\\ntionA multi-purpose ML method with multi-layer perception neural\\nnetwork.\\n[122] Diagnosis of heart disease and detection of heart\\nproblems.A data-driven-based analysis framework with traditional classiﬁcation\\nalgorithms.\\n[123] Prediction of complete future system states relying\\nDT of urban farming.Several common ML algorithms for regression (LR) and classiﬁca-\\ntion (SVM).\\n[124] Improvement of resource utilization in edge\\ncomputing-aided IoVs network.Deep Q-network optimized the function approximation of DL and\\nRL.\\n[126] Simulation of the operation state and analysis real-\\ntime behavior via DT.FL framework with deep RL model.\\nNeural Interface [129] ERP classiﬁcation in BCI systems. Spatiotemporal feature extraction and ML algorithms (e.g., LR, Naive\\nBayes, and SVM).\\n[130] Spatial object localization in aerial images. Feature selection with SVM classiﬁcation.\\n[132] ERP detection in BCI systems. Capsule network with primary capsule components.\\n[134] Brain-controlled robotic arm system. DL framework with multi-directional CNN and bidirectional LSTM.\\n[136] EEG-based classiﬁcation tasks in BCI systems. EEG-Inception network with cross-subject transfer learning and ﬁne-\\ntuning.14\\nHealthcareManufacturingSmart\\nCitiesGaming\\nPhysical activity recognition\\nSensor-based fall detection\\nLesion segmentation in breast image\\nNon-rigid heart motion estimation\\nLiving assistance and risk awareness\\nVirtual health centers and hospital\\nTreatment planning and educative trainingShortening product lifecycle\\nMachine condition supervision\\nFault detection and diagnosis\\xa0\\nProduction line optimization\\nManufacturing scalability and compatibility\\nMake-to-order manufacturing enterprise\\nVirtual entities for operating transparency\\nIntelligent transport system\\nSmart community portal\\nVideo-based surveillance system\\nCollaborative home appliances control\\nSmart environmental trackind and awareness\\nSustainable green agriculture\\nVirtual replication in metaverse ecosystemConsole, mobile, and PC gaming platforms\\nAI-assisted game store telling\\nProcedural content creation\\nTactical planning for AI agent\\nImmersive gamming experience evaluation\\nAI-aided gaming developing optimization\\nRealistic player-NPC interactionDevelopment of virtual stores\\xa0\\nShopping experience improvement\\nPersonalizing customer experience\\nShopping behavior analysis and understandingE-commerce\\nVirtual job fairs\\nImmersive recruitment experience\\nRevolution of working style and workplace\\nVirtual meeting platform supporting metaverseHuman Resource\\nVirtual land investment in the metaverse\\nLand and house for trading\\nNFT-associated real estate in virtual worlds\\nCost-efficient marketing channel for real estate companiesReal EstateCryptocurrency-based financial platform\\nLeaning, borrowing, farming, and staking\\nDecentralized exchange and application\\nTrading products and NFT using cryptoDeFi\\nMetaverse\\'s\\nApplications\\nFig. 11. AI for the metaverse in the application aspects with healthcare, manufacturing, smart cities, and gaming besides other promising domains, such as\\nE-commerce, human resources, real estate, and DeFi.\\nlytics, several healthcare and medical diagnosis applications\\n(e.g., motor rehabilitation and magnetic resonance imaging\\nneurofeedback) can be developed in the VR environment\\nfor multipurpose, such as collaborative treatment planning\\nand educative training [147]. Indeed, several healthcare and\\nmedical services can be provided in the metaverse. For ex-\\nample, medical students can improve surgical skills by doing\\ninteractive practice lessons built for medical education in the\\nvirtual world or patients can ﬁnd some healthcare services via\\nvirtual assistants at virtual health centers and hospitals.\\nB. Manufacturing\\nAs the current wave of industrial revolution, digital trans-\\nformation in manufacturing has been happening with digital\\nconnection between machines and systems to better analyze\\nand understand the physical entities. Different from digital\\ntransformation to enhance the physical world via digital oper-\\nations, the metaverse creates a virtual world that is translated\\nonto the physical world based on the foundation of reality in-\\nteraction and persistence. By collaboratively adopting cutting-\\nedge technologies, such as AI and DT, the metaverse for man-\\nufacturing can signiﬁcantly modernize digital operations in the\\ncurrent digital revolution. Currently, AI with ML algorithms\\nand DL architectures have considerably contributed to the\\nmanufacturing domain via numerous industrial applications.\\nIn manufacturing, shortening product lifecycles and increas-\\ning the number of product variants are the main reason of highexpense for frequent production system reconﬁgurations and\\nupgrades, especially with ML-based systems which have spent\\nmore time and computing resources for new data collection,\\npreprocessing, and model learning. To overcome the above\\nchallenges, a symbiotic human-ML framework [148] was\\nleveraged with a reinforcement learning strategy by combining\\nthe learning capacity of Q-learning models and the domain\\nknowledge of experts. This framework also considered human\\nexploration to reduce noise in data and improve the quality\\nof automatic decision-making systems. As a great importance\\nin the modern manufacturing systems, quality inspection has\\nbeen recently attracting more attention with intelligent data-\\ndriven condition supervision approaches; however, they had\\nto face some difﬁculties from different operating conditions\\nwith diversiﬁed tasks and applications [149]. For reliable fault\\ndetection and diagnosis in manufacturing, numerous methods\\nhave exploited DL with RNN and CNN architectures to\\nachieve high accuracy while keeping a real-time monitoring.\\nFor instance, a RNN [150] was developed with an encoder-\\ndecoder structure coupled with attention mechanism to pre-\\ndict and diagnose interturn short-circuit faults in permanent\\nmagnet synchronous systems. In [151], a data-driven LSTM-\\nbased fault diagnosis approach was introduced to early detect\\nmultiple open-circuit faults in wind turbine systems. In [152],\\na DL-based intelligent fault diagnosis method was introduced\\nto address two challenging problems, i.e., the lack of labeled\\ndata for learning model and the data distribution discrepancy15\\nbetween training and testing sets, by incorporating CNN\\narchitecture and transfer learning mechanism.\\nDesign and implementation of an optimal serial production\\nline are crucial to increase the productivity of the whole\\nmanufacturing process. Many recent works have applied AI\\nto optimize some speciﬁc sectors in a production system\\nand improve the performance of production line accordingly\\nwhile meeting scalability and compatibility. For example, a\\nprediction model [153] was developed to estimate the optimal\\nbuffer size in production lines by combining a regular artiﬁcial\\nneural network (ANN) and a generic algorithm. The prediction\\nmodel was further integrated with an optimization mechanism\\nto evaluate and forecast the optimal buffer size in need to max-\\nimize productivity. In [154], an efﬁcient production progress\\nprediction method was formulated with the combination of\\nDL and IoT to optimize the dynamic production and on-\\ntime order delivery activities in make-to-order manufacturing\\nenterprises. In the proposed method, a two-stage transfer\\nlearning mechanism was executed with historical data and\\nreal-time state data using a deep belief network to solve the\\nnonlinearity of production progress. Nowadays many manufac-\\nturing plants have developed industrial collaborative robots to\\nundertake different advanced tasks which require much more\\ncognitive skills, intelligence, and domain knowledge of human\\nto immediately respond unexpected actions or events with\\nhigh precision and conﬁdence [155]. Therefore, it demands\\na cooperative AI model to learn complicated patterns from\\nmultimodal data for different correlated tasks of manufacturing\\nprocess and production line, in which the AI model should\\nbe equipped with the capability of explanation and reason-\\ning. Through virtual entities in the metaverse, the industrial\\nmanufacturing efﬁciency is generally improved with AI to\\nspeed up production process design, motivate collaborative\\nproduct development, reduce operation risk to quality control,\\nand obtain high transparency for producer and customers.\\nC. Smart Cities\\nSmart cities acquire the meaningful information about the\\nneeds of citizens through the IoT, video cameras, social media,\\nand other sources. Based on the feedbacks automatically\\ncollected from users, city governments need to make decisions\\nabout which services to remove, offer, and improve. By using\\nmore digital tools and pioneering technologies, smart cities\\nwill provide smarter interactive services to users over the\\nmetaverse platform [156]. The environmental data (e.g., air\\nquality, weather, energy consumption, trafﬁc status, and avail-\\nable parking space) are fully replicated in the virtual world\\nfor user-friendly interface. Several smart services, such as\\nutility payment and smart home control, can be now executed\\nin the virtual world via the platforms and systems deployed\\nin the metaverse: intelligent transportation systems (ITS),\\nsmart street light management systems, automatic parking\\nsystems, smart community portals, and indoor/outdoor video\\nsurveillance systems. Currently, the actual impact and beneﬁt\\nof these technologies to the smart cities is limited; however,\\nthe metaverse can be an accelerant to spread the presentation\\nof smart services in the daily life of citizens [157].Among different technologies to enable smart cities in\\nthe physical world and in the metaverse, AI has shown a\\ngreat signiﬁcance to achieve automation and intelligence in\\nsmart services. By integrating EEG-based BCI, VR, and IoT\\ntechnologies fueled by AI, the work in [158] introduced a\\nsteady-state visual evoked potential-based BCI architecture\\nto collaboratively control home appliances. With the visual\\ninformation captured through head-mounted display, the brain\\nsignals were recorded to analyze with ML algorithms and\\nrespond control commands with stimulation, which allow users\\nto control home appliances over the IoT network. In the\\neffort to develop a hybrid ITS from the physical world to\\nits virtual replication in the digital world, the comprehensive\\nwork in [159] introduced an intelligent and ubiquitous IoT-\\nenabled architecture to control and manage urban trafﬁc. Many\\nscenarios with practical data processing and decision making\\nof different transportation services were investigated simulta-\\nneously in both of physical and virtual world, thus conducting\\nhigh-quality real-time services to users and reducing operation\\nand maintenance costs. Because of the hastiness of industrial-\\nization and the explosion of urbanization, air pollution has\\nbecome a life-warning problem, which has affected living\\nenvironment and physical health defectively. For early air\\npollution warning and management, an efﬁcient forecasting\\napproach was studied with a hybrid DL architecture [160],\\nwhich combined 1-D CNNs and bi-directional LSTM net-\\nworks to fully extract intrinsic correlation features and in-\\nterdependence of multivariate time series data acquired from\\nmultiple sensors. Besides environmental pollution, sustainable\\nagriculture has been attracting much more concerns in smart\\ngreen cities [161]. In this context, AI is one of the vital\\ninformation and communication (ICT) technologies, which has\\nbeen widely used in precision agriculture systems with yield\\nprediction, quality evaluation, and pest and disease detection.\\nDesigning and implementing the metaverse ecosystem for\\nsmart cities with all administrative services, such as envi-\\nronment, education, transportation, culture, and other civil\\nservices, is really a challenging mission of metropolitan gov-\\nernment. By gathering big data from multiple authenticated\\nsources, many administrative services can be provided and\\nimproved in the metaverse thanks to AI technology for data\\nanalytics, in which usage rules, ethics, and security will be\\nreleased to guarantee a safe experience environment.\\nD. Gaming\\nGaming has always been a prime application in the meta-\\nverse, in which ML and DL are redeﬁning and revolutionizing\\nthe gaming industry across multiple platforms, from console\\nto mobile and PC platforms. This part will explore how ML\\nand DL can revolutionize game development and how building\\na next gaming generation in the metaverse.\\nIn the last decade, ML has had a huge impact on the way\\nvideo games are developed. To build more realistic worlds\\nwith attractive challenges and unique stories, video game\\ndevelopers and studios have been increasingly turning to ML\\nas a powerful tool set that help systems and NPCs to respond\\nto player’s action dynamically and reasonably. In [166], the16\\nTABLE II\\nSUMMARY OF AI F ORTHEMETAVERSE IN THEAPPLICATION ASPECT .\\nApplication Aspect Ref Description AI Technique\\nHealthcare [139] Inertial sensory -based physical activity recogni-\\ntion for healthcare and wellness.CNNs with an encoding algorithm to convert inertial sensory\\nsignals to color images.\\n[138] Physical activity recognition to support physicians\\nand health-wellness experts in making decisions in\\nliving assistance and healthy risk awareness.Residual CNNs to extract deep features in combination with\\nhandcrafted features based on an intermediate fusion mechanism.\\n[140] Wearable devices based fall detection for IoT-\\nbased healthcare and medical services.Hierarchical DL framework with CNNs for learning models at local\\ndevices and global cloud center.\\n[144] Lesion segmentation in breast ultrasound images\\nfor detecting abnormalities.Saliency-guided morphology-aware U-Net (SMU-Net) involving an\\nadditional middle feature learning stream and an auxiliary network.\\n[145] Estimation of non-rigid motion of heart in free-\\nbreathing 3D coronary magnetic resonance angiog-\\nraphy images.Deep encoder-decoder learning framework with CNN architectures.\\nManufacturing [148] Automatic production system reconﬁgurations and\\nupgrade to shorten product lifecycles and increase\\nthe number of product variants.Symbiotic with reinforcement learning strategy by combining the\\nlearning capacity of Q-learning models and the domain knowledge\\nof experts.\\n[150] Reliable fault detection and diagnosis for quality\\ninspection of production line in manufacturing.Deep encoder-decoder framework using RNN architectures with\\nattention mechanism.\\n[151] Data-driven learning model using deep networks with LSTM ar-\\nchitecture.\\n[152] Data-driven learning model using transfer learning mechanism on\\nCNN architecture.\\n[153] Forecasting the optimal buffer size required in\\nproduction systems to maximize productivity.A combination of a regular artiﬁcial neural network and a generic\\nalgorithm.\\n[154] Production progress prediction to optimize on-\\ntime order delivery activities in in make-to-order\\nmanufacturing enterprises.Data-driven learning model using two-stage transfer learning mech-\\nanism on deep belief network architecture.\\nSmart Cities [158] Smart home appliances control and management\\nover IoT networks.A set of AI algorithms for brain signal processing and decision\\nmaking to respond stimulation commands.\\n[159] Intelligent and ubiquitous IoT-enabled urban trafﬁc\\ncontrol and management in ITS systems.Hierarchical AI framework by deploying ML algorithms in the\\ndigital world to automatically make decisions in the physical world.\\n[160] Early air pollution warning and management in\\nsmart environment surveillance systems.DL framework with 1-D CNNs coupled with bi-directional LSTM\\nnetworks.\\n[161] Sustainable agriculture from smart farm to intelli-\\ngent food production line.AI framework for supervised learning with conventional ML algo-\\nrithms applied in detection, classiﬁcation, and recognition models.\\nGaming [162] Modeling multi-scale uncertainty and multi-level\\nabstraction levels in RTS games.Multi-scale Bayesian models including Bayesian networks and\\nmaps with probabilistic learning for multi-level units control,\\ntactics, and strategy.\\n[163] Design of a metamorphic testing mechanism for\\ngame ﬂow evaluation in artiﬁcial chess games.ML framework with decision tree algorithm to determine the\\noptimal move among all possible ones for AI agents.\\n[164] Development of AI agents in player-vs-NPC and\\nplayer-vs-player ﬁghting games.A combination of RL and deep networks with different architec-\\ntures, such as RNN and CNN.\\n[165] Development of AI agents and improvement of\\ntactical intelligence in RTS games.Supervised learning framework with CNN architectures and rein-\\nforcemen learning with deep Q-learning networks.\\nrole of artiﬁcial and computational intelligence in games has\\nbeen discussed regarding many research topics: NPC behav-\\nior strategy and learning, tactical planning, player response\\nmodeling, procedural content creation, player-NPC interaction\\ndesign, general game AI, AI-assisted game story telling, and\\nAI in commercial games. This work further investigated these\\ntopics according to three viewpoints: AI algorithms used in\\neach topic, effectiveness of AI to human user in every topic,\\nand human-computer interaction. For in-game decision making\\nand learning, a comprehensive survey in [167] investigated\\nthe use of AI algorithms in intelligent video and computer\\ngames. Regarding decision making, some primary AI algo-\\nrithms (such as decision tree, fuzzy logic, Markov model, rule-\\nbased system, and ﬁnite-state machine) were deployed for dif-\\nferent game developing tasks: modeling game ﬂow, assessing\\nplaying motivation, evaluating immersive experience, adapting\\ngameplay, adapting gaming strategy, customizing gameplay,\\nand modeling and controlling NPC behavior. Besides, many\\nlearning-based tasks were accomplished with Na ¨ıve Bayes,ANN, SVM, and case-based reasoning system to classify user\\ngameplay, classify NPC behavior, recognize user behavior, and\\nadapt game ﬂow based on personal experience.\\nIn real-time strategy (RTS) games, such as StarCraft,\\nBayesian models have been used for modeling multi-scale\\nuncertainty and multi-level abstraction levels [162]: micro-\\nmanagement, tactics, and strategy. These probabilistic learning\\nmodels were able to cope reactive units control, recognize\\nobjectives from tactical data, and predict opponent’s gameplay\\nbased on strategic information. To reach an intelligent human-\\nlike response mechanism, several game software companies\\nhas applied AI in various testing tasks during design and de-\\nvelopment stages. In [163], a metamorphic testing mechanism\\nwas proposed to overcome the impracticability of controlling\\na large amount of possible moving strategies in artiﬁcial chess\\ngames. This testing mechanism deployed a decision tree model\\nto reveal metamorphic relations, which in turn effectively\\ndetermine the optimal move among all possible ones. With\\na combination of RL and deep networks, AI agents in [164]17\\nwere developed to address some inherent difﬁculties in real-\\ntime ﬁghting games and to defeat pro players in one-vs-\\none battle. In addition to creating different ﬁghting styles\\nthrough self-play curriculum, this deep RL framework was\\ncapable of all two-player competitive games which have level\\nupgrade and balance policies. RL and supervised learning were\\nalso exploited to improve AI agents in RTS games [165].\\nBeing more superior than the Puppet search algorithm, CNNs\\nand deep Q-learning networks inferred the outcome of costly\\nhigh-level search and optimized the available time to execute\\ntactical searches. In a nutshell, AI with traditional ML and\\ninnovative DL algorithms have been making an unprecedented\\nrevolution of gaming experience in many aspects: improving\\nthe intelligence of NPCs, modeling complex systems, making\\ngames more beautiful and rational, conducting more realistic\\nhuman-NPC interactions, reducing cost of in-game world\\ncreation, and opening more opportunities of developing mobile\\ngames. In Table II, we summarized the existing application-\\noriented works utilizing AI technology which have shown the\\npotential to be integrated and deployed in the metaverse.\\nE. Other Potential Applications\\nBesides healthcare, manufacturing, smart cities, and gaming,\\nwe have found some auxiliary business applications ahead for\\nthe metaverse.\\nE-commerce : For the purpose of which E-commerce has\\nbeen integrated into the metaverse, numerous consumer brands\\nhave been diving into the digital world to create more de-\\nlightful and seamless shopping experiences regardless of the\\nunpopularity of VR devices for mainstream consumers. Many\\nbrands have moved forward step-by-step to build something\\nentirely new by integrating digital stores which are able\\nto bring the best ofﬂine and online shopping without any\\ndifference in user experience [168]. Indeed, virtual shopping\\ncan convey remotely real-time experience of static products,\\nin which consumers, represented by an avatar, can walk\\naround stores in a 3D rendered space and talk with virtual\\ncashier/seller powered by VR and AI technologies. Person-\\nalizing the customer experience is currently attracting more\\nattention from retailers, not only for business survival but also\\nfor revenue growth, which can be performed effortlessly in the\\nmeta with AI-based shopping behavior understanding.\\nHuman resources : Nowadays, many big tech companies\\nare being creative to seek and communicate with young\\ntalents who are looking for jobs. The recruitment manners\\nrange from dispatching younger employees/leaders to online\\ninterview applicants with video calls to holding job fairs in the\\nmetaverse. Potential applicants can login into the metaverse\\nwith blockchain-aided authenticated account and then control\\ntheir avatars to freely discuss with other avatars representing\\nthe company’s human resource managers and project lead-\\ners [169]. For recruitment guidance, the applicants can ask\\nor receive help from the virtual assistant with AI-based NLP.\\nIn these kinds of recruitment events, the goal is to generate\\na friendly environment for both the recruiters and applicants\\nfor free-style communication, in which the applicants can\\nactively discover more information about job positions insteadof passively being asked questions by recruiters. In the last\\ndecade, emerging technologies (such as 5G, IoT, and DL)\\nhave brought workers/employees many convenient alterna-\\ntives (fully remote and hybrid ofﬂine-online) to traditional\\nwork; however, the metaverse will revolutionize the future\\nof work and the workplace. Recently, Facebook introduced\\nHorizon Workrooms [170], a well-designed meeting platform\\nthat allows users, represented as avatars, to work, collaborate,\\nand communicate with others, besides training and coaching\\nactivities, in the virtual space by VR devices.\\nReal estate : We have seen a huge investment from individual\\ninvestors and institutions to virtual land in the metaverse.\\nSome metaverses have already been released, including vir-\\ntual gaming platforms like the Sandbox and Axie Inﬁnity\\nand virtual worlds like Decentraland and Upland [171], in\\nwhich users can buy, sell, and trade things, including real\\nestate (plots of land and virtual houses). These digital real\\nestates, usually associated by non-fungible tokens (NFTs), are\\nlimited by supply to guarantee their values over time based\\non scarcity. The estate in the metaverse can be used as a\\nvirtual place for building constructions (houses and ofﬁces) or\\nholding digital events (e.g., art exhibition and fashion show).\\nMoreover, the metaverse is another cost-efﬁcient channel for\\nreal estate companies to ultimately show the property to clients\\nbefore making decision. With VR-aided immersive experience,\\nthe clients can discover the property, including interior and\\nexterior from detailed furniture and overall structure, via VR\\ntours and interactive walkthroughs.\\nDecentralized ﬁnance : Based on an open system of ﬁnance,\\ndecentralized ﬁnance (DeFi) is a cryptocurrency-based ﬁnan-\\ncial service which is regularly programmed through smart\\ncontracts to build exchanges besides providing many major\\nservices, such as lending, yield farming, and insurance with-\\nout centralized authorities. Different from centralized ﬁnance\\nwhich is controlled or managed by a centralized entity or a\\nperson, DeFi, with blockchain technology, facilitates ﬁnancial\\nservices from peer-to-peer and allows users to fully control\\ntheir assets while ensuring security and privacy. DeFi services\\nare usually delivered via decentralized applications (Dapps)\\nwhich are entirely built on open-sourced distributed platforms.\\nBy integrating DeFi (including basic and professional services)\\ninto the metaverse, users can make purchases virtual products\\nidentiﬁed by NFTs in the digital world, but will receive the\\nreal products in the real life. Furthermore, users can make\\nproﬁts in the metaverse based on the DeFi ecosystem with the\\nlending, borrowing, mining, and staking cryptocurrencies or\\nother tokens. Users can provide liquidity to the liquidity pool\\nwith an underlying AI-based mechanism of a decentralized\\nexchange to earn incentives. Swapping tokens (can belong to\\nthe same chain or different chains) is the basic service that is\\nprioritized to develop ﬁrst on any Dapps.\\nV. M ETAVERSE PROJECTS\\nThis section brieﬂy introduces some attractive metaverse\\nprojects, including Decentraland, Sandbox, Realy, Star Atlas,\\nBit.Country, and DeHealth, which have applied AI to deliver\\nmultifarious services and applications in the virtual world,18\\nFig. 12. Inside the virtual worlds of different metaverse projects (left to right): Decentraland, Sandbox, Realy, Star Atlas, Bit.Country, and DeHealth.\\nfrom real estate to E-commerce and real estate. The landscapes\\ninside the virtual worlds of the projects are shown in Fig. 12,\\nfor DeHealth, virtual doctors as avatars in the metaverse.\\nDecentraland3: This is a decentralized virtual reality plat-\\nform built on the Ethereum blockchain, in which users can\\nexperience, create, and monetize assets, contents, and appli-\\ncations. In Decentraland, a virtual land is determined as a\\nnon-fungible, transferrable, and scare digital asset recorded by\\nthe Ethereum smart contract. Different from traditional virtual\\nworlds and social networks, Decentraland is not controlled\\nby any centralized organization; that is, no single agent has\\na permission to modify the rules of software, content, eco-\\nnomic of cryptocurrency, or prevent others from accessing\\nthe world, trading digital products, and experienced services.\\nThe traversable 3D world in Decentraland allows embedding\\nimmersive component and adjacency to creative content that\\nmakes this project attractive and unique. A scripting pro-\\ngramming language of Decentraland enables developers to\\neasily code AI-based service-oriented applications for users\\nand accordingly encourage users to create new contents. Some\\nprincipal use cases are content curation, advertising, digital\\ncollectibles, and social besides other minor ones, such as\\neducation, virtual tourism, healthcare, and virtual shopping.\\nRegarding architecture, the Decentraland protocol has three\\nlayers: a consensus layer to track land ownership and its\\ncontent, a land content layer to distribute the materials for\\nrendering via a decentralized storage system, and a real-time\\nlayer to establish peer-to-peer connections for world viewing.\\nThe native token, for in-world purchase goods and services, of\\nDecentraland is MANA, a fungible token built on the ERC-20\\n(Ethereum Request for Comments 20) protocol.\\nSandbox4: The Sandbox metaverse is a user-generated de-\\ncentralized Ethereum blockchain-based virtual world, which\\nallows users and gamers to build, own, and monetize gaming\\nexperiences. Inspired by Minecraft5, the Sandbox metaverse\\nis ﬁrst built as a 2D mobile pixel game and then extended\\nto a fully-ﬂedged 3D world with a voxel gaming platform,\\nwherein users are capable of playing, sharing, collecting, and\\ntrading virtual goods and services without central control.\\nRemarkably, creators can earn SAND, the native token of\\nSandbox, by selling their creations on a marketplace with\\nsecure copyright ownership which is associated and guaranteed\\nvia NFT, i.e., every item in the metaverse will be authenticated\\nby a unique and immutable blockchain mechanism. As the\\nprimary use cases of SAND in the Sandbox metaverse, the\\ntoken holders can access and experience the virtual world,\\nvote governance decisions via DAO mechanism, stake tokens\\n3https://decentraland.org/\\n4https://www.sandbox.game/en/\\n5https://www.minecraft.net/en-usto earn revenues, and donate tokens as incentive to developers\\nfor the metaverse growth. Besides the blockchain technology\\nwith ERC-20 to generate SAND tokens and ERC-1155 for\\ndigital assets trading, AI has been utilized in the Sandbox\\nmetaverse. For example, as a powerful toolkit, gaming coders\\ncan deploy ML models to improve the intelligence of virtual\\nagents/assistants and DL models to enhance the render quality,\\nand developers can leverage different AI frameworks to mini-\\nmize gaming crashes and errors. Building and training ML/DL\\nmodels are trouble-free with intuitive high-level APIs.\\nRealy6: From the real world to the fully virtual world by\\ncreating a unique metaverse ecological world, the Realy meta-\\nverse is deﬁned as a super-realistic, futuristic, technologically\\nconscious world, in which E-commerce, social, gaming, and\\ntrading are truly integrated to bring a seamless virtual-reality\\nexperience to users. In the virtual world of Realy metaverse,\\nusers can enjoy colorful journeys through their personalized\\navatars with 3D virtual clothes which are available in the mar-\\nketplace and linked with unique NFTs. There is an interesting\\nfeature of Realy compared with other metaverse projects, that\\nis about the avatar control and management procedures. When\\nusers are online, their avatars bring immersive experience.\\nWhen users are ofﬂine, the avatars are driven by a set AI-\\nbased self-discipline systems. The whole virtual world is\\nautomatically operated, controlled, and managed by a decen-\\ntralized DAO organization. Regarding technology in the Realy\\nmetaverse, besides VR and blockchain, AI is adopted in many\\naspects to generally improve the user immersive experience,\\nsuch as enhancing 3D visual rendered effects, boosting the\\nintelligence of avatars for realistic behaviors, and integrating\\nVR and holographic projection.\\nStar Atlas7: As one of the most recent innovative metaverse\\nprojects, Star Atlas introduces a virtual gaming world built\\non the integration of multiplayer video game platforms, real-\\ntime immersive experiences with 3D rendered visualization,\\nblockchain-based decentralized ﬁnancial services, and AI-\\npowered game engines. Star Atlas helps to complete an\\necosystem on the Solana blockchain by ﬁlling the gap be-\\ntween metaverse and blockchain technology. In the gaming\\nmetaverse of Star Atlas, users can trade digital assets like land,\\nequipment, crew, ship, and components by using the in-game\\ncryptocurrency token known as POLIS which can be used for\\nmultiple cross-metaverse games. To tune the gameplay more\\nlogical and realistic, ML algorithms are applied to improve\\nthe intelligence of NPCs and AI agents in tactical planning of\\nactions and ﬁghting strategy in player-NPC combats.\\nBit.Country8: As a user-oriented metaverse project,\\n6https://realy.pro/\\n7https://staratlas.com/\\n8https://bit.country/19\\nBit.Country builds a 3D virtual world for everyone who can\\nset up its own community in the metaverse with rules and\\noperations to attract followers and contributors. By introducing\\na new level of virtual social interaction, Bit.Country has two\\nplatforms: one is traditional web view for content creation\\nand service provision, and another is addition 3D gaming\\nview for VR-aided immersive experience. In each individual\\ncommunity conﬁgured by users holding platform tokens, all\\nthe rules are managed and linked together by AI models to\\nensure logical operations without any conﬂicts. Being more\\nthan a virtual world, Bit.Country is capable of connecting\\nsome virtual aspects to the real world to obtain a sustainable\\nfuture instead of joyful immersive experience.\\nDeHealth9: Introduced by a British non-proﬁt organiza-\\ntion, DeHealth is the world’s ﬁrst decentralized healthcare\\nmetaverse, which allows doctors and patients to work and\\ninteract with each other in a full 3D virtual world. In\\nthe DeHealth metaverse, several high-quality healthcare and\\nmedical services are delivered, such as health analytics on\\nthe go, recommendations from advanced AI-bot, and real-\\ntime dialogues with doctors and health experts worldwide.\\nTo encourage informative data sharing activities, the meta-\\nverse has some trading cryptocurrency pools for users and\\npatients to earn assets by selling anonymized medical data.\\nThe data collected from a decentralized network will be used\\nto build AI-based diagnostic models for diversiﬁed tasks in\\nthe healthcare and medical domains. In the metaverse, doctors\\nand patients are able to communicate via a virtual space\\nreplicating the real-world environment. Some virtual hospitals\\nand healthcare centers can be built to provide virtual services\\nwith real data and diagnosis results. The DeHealth metaverse\\nis in consideration to be extended with additional education-\\noriented services based on VR technology.\\nVI. C ONCLUSION AND RESEARCH DIRECTIONS\\nIn this survey, we have comprehensively investigated the\\nrole of AI in the foundation of the metaverse and its potential\\nto enhance user immersive experience in the virtual world.\\nAt the beginning of this work, the fundamental concepts of\\nthe metaverse and AI techniques have been provided, along\\nwith the role of AI in the metaverse. Subsequently, several\\nprincipal technical aspects, such as NLP, machine vision,\\nblockchain, networking, DT, and neural interface, and many\\napplication aspects, such as healthcare, manufacturing, smart\\ncities, gaming, E-commerce, and DeFi, have been analyzed.\\nThe reviewed AI-based solutions have shown that AI has\\ngreat potential to toughen the systems infrastructure, uplift\\nthe 3D immersive experience, and ﬂourish the built-in services\\nin the virtual worlds signiﬁcantly. Finally, we have examined\\nprominent metaverse projects, in which AI techniques were\\nused to sharpen the quality of services and encompass the\\necosystem of the metaverse.\\nWe now delineate some AI research directions in the\\nmetaverse. Being more advanced than regular virtual per-\\nsonal assistants which are developed for a general purpose\\nwith simple dialog management, virtual customers/employee\\n9https://www.dehealth.world/\\nAutomated Speech\\nRecognition\\nListening\\nNatual Language\\nUnderstanding\\nComprehendingDialog\\nManagement\\nForming ResponseNatual Language\\nGeneration\\nOffering response\\nHi, I need to\\naccess a virtual\\nmeeting.Sure, I need your\\nID to check the\\naccess\\npermission?\\xa0\\nMachine Learning and Deep Neural NetworksFig. 13. General processing ﬂow of conversational AI to deliver contextual\\nand personal experience to users.\\nassistants powered by conversational AI can serve many\\nspeciﬁc purposes of multi-level philosophical conversations to\\nenhance user interactive experience. Conversational AI with\\na processing ﬂow in Fig. 13 is a set of technologies (e.g.,\\nautomatic speech recognition, language processing, advanced\\ndialog management, and ML) that can offer human-like in-\\nteractions in the metaverse based on recognizing speech and\\ntext, understanding intention, deciphering various languages,\\nand responding human-mimicking conversations over voice\\nmodality.\\nMost of the current metaverse projects limit users to ex-\\nplore, own, and customize things in the virtual world. In\\nthe future, users will be allowed to create hyperreal objects\\nand content easily and quickly with the help of AI. Various\\nkinds of hyperreal objects (e.g., faces, bodies, plants, ani-\\nmals, vehicles, buildings, and other inanimate objects) can be\\nremixed endlessly by users to make unique experiences and\\nexcite creation. Accordingly, the combination of VR and AI-\\nbased content generation can bring a complete immersion in\\nalternative realities. In this context, AI tools should be cheap\\nto everyone and have user-friendly interfaces. Further, ethical\\nissues relating to user-generated metaverse need to be seriously\\nexamined with constraints and policies between users and\\nthird-party organizations to mitigate risks and harmful threats\\nto individuals and societies when users synthesize hyperreal\\nmedia contents.\\nIn many AI-aided services and applications in the meta-\\nverse, the decisions are made by AI agents, which are driven\\nby ML models as black boxes without the capability of in-\\nterpretability and explainability. Metaverse developers, virtual\\nworld designers, and users cannot completely understand AI\\ndecision-making processes (e.g., how and why an AI model\\ndelivers a prediction), and probably trust them blindly. To\\novercome these problems, explainable AI (XAI) is a set of\\ntools and methods to describe AI models, analyze their ex-\\npected impacts, characterize model transparency, and examine\\noutcomes, allowing human users to entirely comprehend and\\ntrust the AI models with end-to-end process monitoring and\\naccountability. With XAI, system engineers and data scientists\\nwho apply AI in the metaverse (from system infrastructure to\\nservices and applications in the virtual worlds) can understand\\nand explain what exactly is happening inside an AI model, how\\nis a speciﬁc result generated by an AI algorithm, and when is\\na prediction model likely to crash. Besides increasing end user20\\nconﬁdence, model auditability, and operative efﬁciency, XAI\\nmitigates legal risks and security threats of production AI in\\nthe metaverse while guaranteeing users’ reliable experience.\\nREFERENCES\\n[1] M. Kanterman and N. Naidu, “Metaverse may be\\n$800 billion market, next tech platform,” Dec. 2021.\\n[Online]. Available: https://www.bloomberg.com/professional/blog/\\nmetaverse-may-be-800-billion-market-next-tech-platform/\\n[2] Wikipedia, “Metaverse.” [Online]. Available: https://en.wikipedia.org/\\nwiki/Metaverse\\n[3] S.-M. Park and Y .-G. Kim, “A metaverse: Taxonomy, components,\\napplications, and open challenges,” IEEE Access , pp. 4209–4251, 2022.\\n[4] J. Radoff, “The metaverse value-chain,” Apr. 2021.\\n[Online]. Available: https://medium.com/building-the-metaverse/\\nthe-metaverse-value-chain-afcf9e09e3a7\\n[5] S. B. Kotsiantis, I. Zaharakis, P. Pintelas et al. , “Supervised machine\\nlearning: A review of classiﬁcation techniques,” Emerging Artiﬁcial\\nIntelligence Applications in Computer Engineering , vol. 160, no. 1,\\npp. 3–24, Jun. 2007.\\n[6] F. Hu, G.-S. Xia, Z. Wang, X. Huang, L. Zhang, and H. Sun, “Unsu-\\npervised feature learning via spectral clustering of multidimensional\\npatches for remotely sensed scene classiﬁcation,” IEEE Journal of\\nSelected Topics in Applied Earth Observations and Remote Sensing ,\\nvol. 8, no. 5, May 2015.\\n[7] R. Sheikhpour, M. A. Sarram, S. Gharaghani, and M. A. Z. Chahooki,\\n“A survey on semi-supervised feature selection methods,” Pattern\\nRecognition , vol. 64, pp. 141–158, Apr. 2017.\\n[8] J. E. Van Engelen and H. H. Hoos, “A survey on semi-supervised\\nlearning,” Machine Learning , vol. 109, no. 2, pp. 373–440, 2020.\\n[9] B. Kiumarsi, K. G. Vamvoudakis, H. Modares, and F. L. Lewis,\\n“Optimal and autonomous control using reinforcement learning: A\\nsurvey,” IEEE Transactions on Neural Networks and Learning Systems ,\\nvol. 29, no. 6, pp. 2042–2062, Jun. 2018.\\n[10] W. Chen, X. Qiu, T. Cai, H.-N. Dai, Z. Zheng, and Y . Zhang, “Deep\\nreinforcement learning for internet of things: A comprehensive survey,”\\nIEEE Communications Surveys &Tutorials , vol. 23, no. 3, pp. 1659–\\n1692, Thirdquarter 2021.\\n[11] C. D. Ho, T.-V . Nguyen, T. Huynh-The, T.-T. Nguyen, D. B. da Costa,\\nand B. An, “Short-packet communications in wireless-powered cogni-\\ntive IoT networks: Performance analysis and deep learning evaluation,”\\nIEEE Transactions on Vehicular Technology , vol. 70, no. 3, pp. 2894–\\n2899, Mar. 2021.\\n[12] C.-H. Hua, T. Huynh-The, and S. Lee, “Dran: Densely reversed atten-\\ntion based convolutional network for diabetic retinopathy detection,” in\\nProc. 42nd Annual International Conference of the IEEE Engineering\\nin Medicine & Biology Society (EMBC) , Montreal, QC, Canada, Jul.\\n2020, pp. 1992–1995.\\n[13] Q.-V . Pham, N. T. Nguyen, T. Huynh-The, L. B. Le, K. Lee, and W.-J.\\nHwang, “Intelligent radio signal processing: A survey,” IEEE Access ,\\nvol. 9, pp. 83 818–83 850, 2021.\\n[14] V . S. Lalapura, J. Amudha, and H. S. Satheesh, “Recurrent neural\\nnetworks for edge intelligence: A survey,” ACM Computing Surveys\\n(CSUR) , vol. 54, no. 4, pp. 1–38, May 2022.\\n[15] R. C. Gonzalez, “Deep convolutional neural networks [lecture notes],”\\nIEEE Signal Processing Magazine , vol. 35, no. 6, pp. 79–87, Nov.\\n2018.\\n[16] A. D. Ramos, E. L ´opez-Rubio, and E. J. Palomo, “The forbidden region\\nself-organizing map neural network,” IEEE Transactions on Neural\\nNetworks and Learning Systems , vol. 31, no. 1, pp. 201–211, Jan. 2020.\\n[17] L. Jing and Y . Tian, “Self-supervised visual feature learning with deep\\nneural networks: A survey,” IEEE Transactions on Pattern Analysis\\nand Machine Intelligence , vol. 43, no. 11, pp. 4037–4058, Nov. 2021.\\n[18] J. Song, H. Zhang, X. Li, L. Gao, M. Wang, and R. Hong, “Self-\\nsupervised video hashing with hierarchical binary auto-encoder,” IEEE\\nTransactions on Image Processing , vol. 27, no. 7, pp. 3210–3221, Jul.\\n2018.\\n[19] S. K. Moore, “Meta aims to build the world’s fastest ai\\nsupercomputer,” Jan. 2022. [Online]. Available: https://spectrum.ieee.\\norg/meta-ai-supercomputer\\n[20] M. Daniluk, T. Rockt ¨aschel, J. Welbl, and S. Riedel, “Frustratingly\\nshort attention spans in neural language modeling,” arXiv preprint\\narXiv:1702.04521 , 2017.[21] K. Benes, M. K. Baskar, and L. Burget, “Residual memory networks\\nin language modeling: Improving the reputation of feed-forward net-\\nworks,” in INTERSPEECH , Stockholm, Sweden, Aug. 2017, pp. 284–\\n288.\\n[22] R. Jozefowicz, W. Zaremba, and I. Sutskever, “An empirical exploration\\nof recurrent network architectures,” in Proc. International Conference\\non Machine Learning , Lille, France, Jul. 2015, pp. 2342–2350.\\n[23] N.-Q. Pham, G. Kruszewski, and G. Boleda, “Convolutional neural\\nnetwork language models,” in Proc. Conference on Empirical Methods\\nin Natural Language Processing , Austin, Texas, Nov. 2016, pp. 1153–\\n1162.\\n[24] B. Liu and G. Yin, “Chinese document classiﬁcation with bi-directional\\nconvolutional language model,” in Proc. 43rd International ACM SIGIR\\nConference on Research and Development in Information Retrieval ,\\nNew York, NY , United States, Jul. 2020, pp. 1785–1788.\\n[25] B. Athiwaratkun and J. W. Stokes, “Malware classiﬁcation with LSTM\\nand GRU language models and a character-level CNN,” in Proc. IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP) , New Orleans, LA, USA, Jun. 2017, pp. 2482–2486.\\n[26] W. Ma, Y . Cui, C. Si, T. Liu, S. Wang, and G. Hu, “Char-\\nBERT: Character-aware pre-trained language model,” arXiv preprint\\narXiv:2011.01513 , 2020.\\n[27] R. Sharma, S. Morwal, B. Agarwal, R. Chandra, and M. S. Khan,\\n“A deep neural network-based model for named entity recognition for\\nhindi language,” Neural Computing and Applications , vol. 32, no. 20,\\npp. 16 191–16 203, Apr. 2020.\\n[28] K. Khysru, D. Jin, Y . Huang, H. Feng, and J. Dang, “A tibetan language\\nmodel that considers the relationship between sufﬁxes and functional\\nwords,” IEEE Signal Processing Letters , vol. 28, pp. 459–463, Feb.\\n2021.\\n[29] N. Jin, J. Wu, X. Ma, K. Yan, and Y . Mo, “Multi-task learning model\\nbased on multi-scale CNN and LSTM for sentiment classiﬁcation,”\\nIEEE Access , vol. 8, pp. 77 060–77 072, 2020.\\n[30] J. Wang, L.-C. Yu, K. R. Lai, and X. Zhang, “Tree-structured regional\\nCNN-LSTM model for dimensional sentiment analysis,” IEEE/ACM\\nTransactions on Audio, Speech, and Language Processing , vol. 28, pp.\\n581–591, Dec. 2019.\\n[31] D. Liu, J. Fu, Q. Qu, and J. Lv, “BFGAN: backward and forward\\ngenerative adversarial networks for lexically constrained sentence gen-\\neration,” IEEE/ACM Transactions on Audio, Speech, and Language\\nProcessing , vol. 27, no. 12, pp. 2350–2361, Dec. 2019.\\n[32] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\\ndeep learning based natural language processing [review article],” IEEE\\nComputational Intelligence Magazine , vol. 13, no. 3, pp. 55–75, Aug.\\n2018.\\n[33] Z. Hu, A. Bulling, S. Li, and G. Wang, “Fixationnet: Forecasting eye\\nﬁxations in task-oriented virtual environments,” IEEE Transactions on\\nVisualization and Computer Graphics , vol. 27, no. 5, pp. 2681–2690,\\nMay 2021.\\n[34] D.-M. Pham, “Human identiﬁcation using neural network-based clas-\\nsiﬁcation of periodic behaviors in virtual reality,” in Proc. IEEE\\nConference on Virtual Reality and 3D User Interfaces (VR) , Tuebin-\\ngen/Reutlingen, Germany, Mar. 2018, pp. 657–658.\\n[35] M. Zhu, Z. Sun, Z. Zhang, Q. Shi, T. Chen, H. Liu, and C. Lee,\\n“Sensory-glove-based human machine interface for augmented reality\\n(AR) applications,” in Proc. IEEE 33rd International Conference on\\nMicro Electro Mechanical Systems (MEMS) , Vancouver, BC, Canada,\\nApr. 2020, pp. 16–19.\\n[36] K. Kataoka, T. Yamamoto, M. Otsuki, F. Shibata, and A. Kimura, “A\\nnew interactive haptic device for getting physical contact feeling of\\nvirtual objects,” in Proc. IEEE Conference on Virtual Reality and 3D\\nUser Interfaces (VR) , Osaka, Japan, Aug. 2019, pp. 1323–1324.\\n[37] P. Wu, W. Ding, Z. You, and P. An, “Virtual reality video quality\\nassessment based on 3D convolutional neural networks,” in Proc. IEEE\\nInternational Conference on Image Processing (ICIP) , Taipei, Taiwan,\\nAug. 2019, pp. 3187–3191.\\n[38] J. Yang, T. Liu, B. Jiang, H. Song, and W. Lu, “3D panoramic virtual\\nreality video quality assessment based on 3D convolutional neural\\nnetworks,” IEEE Access , vol. 6, pp. 38 669–38 682, 2018.\\n[39] Y . Jin, M. Chen, T. Goodall, A. Patney, and A. C. Bovik, “Subjective\\nand objective quality assessment of 2D and 3D foveated video com-\\npression in virtual reality,” IEEE Transactions on Image Processing ,\\npp. 5905–5919, Jun. 2021.\\n[40] Y . Han, C. Yu, D. Li, J. Zhang, and Y . Lai, “Accuracy analysis on 360°\\nvirtual reality video quality assessment methods,” in Proc. IEEE/ACM\\n13th International Conference on Utility and Cloud Computing (UCC) ,\\nLeicester, UK, Dec. 2020, pp. 414–419.21\\n[41] E. Barba, B. MacIntyre, and E. D. Mynatt, “Here we are! where are\\nwe? locating mixed reality in the age of the smartphone,” Proceedings\\nof the IEEE , vol. 100, no. 4, pp. 929–936, Apr. 2012.\\n[42] M. Kersten-Oertel, P. Jannin, and D. L. Collins, “DVV: a taxonomy\\nfor mixed reality visualization in image guided surgery,” IEEE Trans-\\nactions on Visualization and Computer Graphics , vol. 18, no. 2, pp.\\n332–352, Feb. 2012.\\n[43] D. Liu, M. Bober, and J. Kittler, “Visual semantic information pur-\\nsuit: A survey,” IEEE Transactions on Pattern Analysis and Machine\\nIntelligence , vol. 43, no. 4, pp. 1404–1422, Apr. 2021.\\n[44] T. Huynh-The, O. Banos, S. Lee, B. H. Kang, E.-S. Kim, and T. Le-\\nTien, “NIC: A robust background extraction algorithm for foreground\\ndetection in dynamic scenes,” IEEE Transactions on Circuits and\\nSystems for Video Technology , vol. 27, no. 7, pp. 1478–1490, Jul. 2017.\\n[45] S. Minaee, Y . Y . Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and\\nD. Terzopoulos, “Image segmentation using deep learning: A survey,”\\nIEEE Transactions on Pattern Analysis and Machine Intelligence , Feb.\\n2021, (Early Access).\\n[46] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” in Proc. IEEE Conference on Computer\\nVision and Pattern Recognition , Boston, MA, USA, Oct. 2015, pp.\\n3431–3440.\\n[47] E. Shelhamer, J. Long, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , vol. 39, no. 4, pp. 640–651, Apr. 2017.\\n[48] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\\n“Deeplab: Semantic image segmentation with deep convolutional nets,\\natrous convolution, and fully connected CRFs,” IEEE Transactions on\\nPattern Analysis and Machine Intelligence , vol. 40, no. 4, pp. 834–848,\\nApr. 2018.\\n[49] Y . Wei, X. Liang, Y . Chen, X. Shen, M.-M. Cheng, J. Feng, Y . Zhao,\\nand S. Yan, “STC: A simple to complex framework for weakly-\\nsupervised semantic segmentation,” IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence , vol. 39, no. 11, pp. 2314–2320,\\nNov. 2017.\\n[50] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:\\nRedesigning skip connections to exploit multiscale features in image\\nsegmentation,” IEEE Transactions on Medical Imaging , vol. 39, no. 6,\\npp. 1856–1867, Jun. 2020.\\n[51] C.-H. Hua, T. Huynh-The, S.-H. Bae, and S. Lee, “Cross-attentional\\nbracket-shaped convolutional network for semantic image segmenta-\\ntion,” Information Sciences , vol. 539, pp. 277–294, Oct. 2020.\\n[52] C.-H. Hua, T. Huynh-The, and S. Lee, “Convolutional networks with\\nbracket-style decoder for semantic scene segmentation,” in Proc. IEEE\\nInternational Conference on Systems, Man, and Cybernetics (SMC) ,\\nMiyazaki, Japan, Jan. 2018, pp. 2980–2985.\\n[53] Y . Liu, K. Chen, C. Liu, Z. Qin, Z. Luo, and J. Wang, “Structured\\nknowledge distillation for semantic segmentation,” in Proc. IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , Long Beach,\\nCA, USA, Jan. 2019, pp. 2604–2613.\\n[54] K.-W. Cheng, Y .-T. Chen, and W.-H. Fang, “Improved object detection\\nwith iterative localization reﬁnement in convolutional neural networks,”\\nIEEE Transactions on Circuits and Systems for Video Technology ,\\nvol. 28, no. 9, pp. 2261–2275, Sep. 2018.\\n[55] Y . Zhu, C. Zhao, H. Guo, J. Wang, X. Zhao, and H. Lu, “Attention\\ncouplenet: Fully convolutional attention coupling network for object\\ndetection,” IEEE Transactions on Image Processing , vol. 28, no. 1, pp.\\n113–126, Jan. 2019.\\n[56] N. Liu, J. Han, and M.-H. Yang, “Picanet: Pixel-wise contextual\\nattention learning for accurate saliency detection,” IEEE Transactions\\non Image Processing , vol. 29, pp. 6438–6451, Apr. 2020.\\n[57] Y . Tang, J. Wang, X. Wang, B. Gao, E. Dellandr ´ea, R. Gaizauskas, and\\nL. Chen, “Visual and semantic knowledge transfer for large scale semi-\\nsupervised object detection,” IEEE Transactions on Pattern Analysis\\nand Machine Intelligence , vol. 40, no. 12, pp. 3045–3058, Dec. 2018.\\n[58] X. Chen, K. Kundu, Y . Zhu, H. Ma, S. Fidler, and R. Urtasun, “3D\\nobject proposals using stereo imagery for accurate object class detec-\\ntion,” IEEE Transactions on Pattern Analysis and Machine Intelligence ,\\nvol. 40, no. 5, pp. 1259–1272, May 2018.\\n[59] M. Feng, S. Z. Gilani, Y . Wang, L. Zhang, and A. Mian, “Relation\\ngraph network for 3D object detection in point clouds,” IEEE Trans-\\nactions on Image Processing , vol. 30, pp. 92–107, Oct. 2021.\\n[60] P. K. Lai, S. Xie, J. Lang, and R. Lagani `ere, “Real-time panoramic\\ndepth maps from omni-directional stereo images for 6 DoF videos in\\nvirtual reality,” in Proc. IEEE Conference on Virtual Reality and 3D\\nUser Interfaces (VR) , Osaka, Japan, Aug. 2019, pp. 405–412.\\n[61] C.-H. Yeh, C.-H. Huang, and L.-W. Kang, “Multi-scale deep residual\\nlearning-based single image haze removal via image decomposition,”IEEE Transactions on Image Processing , vol. 29, pp. 3153–3167, Dec.\\n2019.\\n[62] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional net-\\nworks for biomedical image segmentation,” in Proc. International\\nConference on Medical Image Computing and Computer-Assisted\\nIntervention . Munich, Germany: Springer, Oct. 2015, pp. 234–241.\\n[63] A. Wang, W. Wang, J. Liu, and N. Gu, “AIPNet: Image-to-image\\nsingle image dehazing with atmospheric illumination prior,” IEEE\\nTransactions on Image Processing , vol. 28, no. 1, pp. 381–393, Jan.\\n2019.\\n[64] Z. Jin, M. Z. Iqbal, D. Bobkov, W. Zou, X. Li, and E. Steinbach, “A\\nﬂexible deep CNN framework for image restoration,” IEEE Transac-\\ntions on Multimedia , vol. 22, no. 4, pp. 1055–1068, Apr. 2020.\\n[65] Y . Zhang, Y . Tian, Y . Kong, B. Zhong, and Y . Fu, “Residual dense\\nnetwork for image restoration,” IEEE Transactions on Pattern Analysis\\nand Machine Intelligence , vol. 43, no. 7, pp. 2480–2495, Jul. 2021.\\n[66] S.-A. Wang, “Research on fuzzy image reconstruction method based\\non real-time fusion technology of VR and AR,” in Proc. International\\nConference on Virtual Reality and Intelligent Systems (ICVRIS) , Jishou,\\nChina, Sep. 2019, pp. 47–50.\\n[67] P. Saalfeld, C. B ¨ottcher, F. Klink, and B. Preim, “VR system for the\\nrestoration of broken cultural artifacts on the example of a funerary\\nmonument,” in Proc. IEEE Virtual Reality and 3D User Interfaces\\n(VR) , Lisboa, Portugal, Apr. 2021, pp. 739–748.\\n[68] A. Lahiri, S. Bairagya, S. Bera, S. Haldar, and P. K. Biswas,\\n“Lightweight modules for efﬁcient deep learning based image restora-\\ntion,” IEEE Transactions on Circuits and Systems for Video Technology ,\\nvol. 31, no. 4, pp. 1395–1410, Apr. 2020.\\n[69] T. Huynh-The, B.-V . Le, S. Lee, T. Le-Tien, Y . Yoon et al. , “Using\\nweighted dynamic range for histogram equalization to improve the\\nimage contrast,” EURASIP Journal on Image and Video Processing ,\\nvol. 2014, no. 1, pp. 1–17, Sep. 2014.\\n[70] W. Zhang, L. Dong, X. Pan, P. Zou, L. Qin, and W. Xu, “A survey\\nof restoration and enhancement for underwater images,” IEEE Access ,\\nvol. 7, pp. 182 259–182 279, 2019.\\n[71] V . Syrris, S. Ferri, D. Ehrlich, and M. Pesaresi, “Image enhancement\\nand feature extraction based on low-resolution satellite data,” IEEE\\nJournal Of Selected Topics In Applied Earth Observations And Remote\\nSensing , vol. 8, no. 5, pp. 1986–1995, May 2015.\\n[72] J. Wang and Y . Hu, “An improved enhancement algorithm based on\\nCNN applicable for weak contrast images,” IEEE Access , vol. 8, pp.\\n8459–8476, 2020.\\n[73] S. Mei, R. Jiang, X. Li, and Q. Du, “Spatial and spectral joint super-\\nresolution using convolutional neural network,” IEEE Transactions on\\nGeoscience and Remote Sensing , vol. 58, no. 7, pp. 4590–4603, 2020.\\n[74] J. Lee, J. Lee, and H.-J. Yoo, “SRNPU: An energy-efﬁcient CNN-based\\nsuper-resolution processor with tile-based selective super-resolution in\\nmobile devices,” IEEE Journal on Emerging and Selected Topics in\\nCircuits and Systems , vol. 10, no. 3, pp. 320–334, 2020.\\n[75] J. Yang, L. Xiao, Y .-Q. Zhao, and J. C.-W. Chan, “Hybrid local and\\nnonlocal 3-D attentive CNN for hyperspectral image super-resolution,”\\nIEEE Geoscience and Remote Sensing Letters , 2020.\\n[76] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using\\ndeep convolutional networks,” IEEE Transactions on Pattern Analysis\\nand Machine Intelligence , vol. 38, no. 2, pp. 295–307, Feb. 2016.\\n[77] J. Yang, J. Wright, T. S. Huang, and Y . Ma, “Image super-resolution\\nvia sparse representation,” IEEE Transactions on Image Processing ,\\nvol. 19, no. 11, pp. 2861–2873, Nov. 2010.\\n[78] Q. Dang, J. Yin, B. Wang, and W. Zheng, “Deep learning based 2D\\nhuman pose estimation: A survey,” Tsinghua Science and Technology ,\\nvol. 24, no. 6, pp. 663–676, Dec. 2019.\\n[79] C. Zheng, W. Wu, T. Yang, S. Zhu, C. Chen, R. Liu, J. Shen,\\nN. Kehtarnavaz, and M. Shah, “Deep learning-based human pose\\nestimation: A survey,” arXiv preprint arXiv:2012.13392 , 2020.\\n[80] M. Ramanathan, W.-Y . Yau, and E. K. Teoh, “Human action recognition\\nwith video data: research and evaluation challenges,” IEEE Transac-\\ntions on Human-Machine Systems , vol. 44, no. 5, pp. 650–663, Oct.\\n2014.\\n[81] K. Chen, S. Gong, and T. Xiang, “Human pose estimation using\\nstructural support vector machines,” in Proc. 2011 IEEE Interna-\\ntional Conference on Computer Vision Workshops (ICCV Workshops) ,\\nBarcelona, Spain, Nov. 2011, pp. 846–851.\\n[82] G. Rogez, P. Weinzaepfel, and C. Schmid, “LCR-net++: Multi-person\\n2D and 3D pose detection in natural images,” IEEE Transactions on\\nPattern Analysis and Machine Intelligence , vol. 42, no. 5, pp. 1146–\\n1161, May 2020.22\\n[83] H. He, G. Liu, X. Zhu, L. He, and G. Tian, “Interacting multiple model-\\nbased human pose estimation using a distributed 3D camera network,”\\nIEEE Sensors Journal , vol. 19, no. 22, pp. 10 584–10 590, Nov. 2019.\\n[84] L. Zhao, J. Xu, C. Gong, J. Yang, W. Zuo, and X. Gao, “Learning to\\nacquire the quality of human pose estimation,” IEEE Transactions on\\nCircuits and Systems for Video Technology , vol. 31, no. 4, pp. 1555–\\n1568, Apr. 2021.\\n[85] T. Huynh-The, B.-V . Le, S. Lee, and Y . Yoon, “Interactive activity\\nrecognition using pose-based spatio–temporal relation features and\\nfour-level pachinko allocation model,” Information Sciences , vol. 369,\\npp. 317–333, Nov. 2016.\\n[86] N. A. Tu, T. Huynh-The, K. U. Khan, and Y .-K. Lee, “ML-HDP:\\nA hierarchical bayesian nonparametric model for recognizing human\\nactions in video,” IEEE Transactions on Circuits and Systems for Video\\nTechnology , vol. 29, no. 3, pp. 800–814, Mar. 2019.\\n[87] T. Huynh-The, C.-H. Hua, N. A. Tu, T. Hur, J. Bang, D. Kim, M. B.\\nAmin, B. H. Kang, H. Seung, S.-Y . Shin et al. , “Hierarchical topic\\nmodeling with pose-transition feature for action recognition using 3D\\nskeleton data,” Information Sciences , vol. 444, pp. 20–35, May 2018.\\n[88] X. Wang, L. Gao, J. Song, and H. Shen, “Beyond frame-level CNN:\\nsaliency-aware 3-D CNN with LSTM for video action recognition,”\\nIEEE Signal Processing Letters , vol. 24, no. 4, pp. 510–514, Apr. 2017.\\n[89] T. Huynh-The, C.-H. Hua, and D.-S. Kim, “Encoding pose features\\nto images with data augmentation for 3-D action recognition,” IEEE\\nTransactions on Industrial Informatics , vol. 16, no. 5, pp. 3100–3111,\\nMay 2020.\\n[90] T. Huynh-The, C.-H. Hua, N. A. Tu, and D.-S. Kim, “Learning\\n3D spatiotemporal gait feature by convolutional network for person\\nidentiﬁcation,” Neurocomputing , vol. 397, pp. 192–202, Jul. 2020.\\n[91] Z. Hu, S. Li, C. Zhang, K. Yi, G. Wang, and D. Manocha, “DGaze:\\nCNN-based gaze prediction in dynamic scenes,” IEEE Transactions on\\nVisualization and Computer Graphics , vol. 26, no. 5, pp. 1902–1911,\\nMay 2020.\\n[92] T. R. Gadekallu, Q.-V . Pham, D. C. Nguyen, P. K. R. Maddikunta,\\nN. Deepa, B. Prabadevi, P. N. Pathirana, J. Zhao, and W.-J. Hwang,\\n“Blockchain for edge of things: applications, opportunities, and chal-\\nlenges,” IEEE Internet of Things Journal , vol. 9, no. 2, pp. 964–988,\\nJan. 2022.\\n[93] A. Cannav `o and F. Lamberti, “How blockchain, virtual reality, and aug-\\nmented reality are converging, and why,” IEEE Consumer Electronics\\nMagazine , vol. 10, no. 5, pp. 6–13, Sep. 2021.\\n[94] Y . Liu, F. R. Yu, X. Li, H. Ji, and V . C. M. Leung, “Blockchain and\\nmachine learning for communications and networking systems,” IEEE\\nCommunications Surveys & Tutorials , vol. 22, no. 2, pp. 1392–1431,\\nSecondquarter 2020.\\n[95] S. Tanwar, Q. Bhatia, P. Patel, A. Kumari, P. K. Singh, and W.-C. Hong,\\n“Machine learning adoption in blockchain-based smart applications:\\nThe challenges, and a way forward,” IEEE Access , vol. 8, pp. 474–\\n488, 2019.\\n[96] P. Kumar, R. Kumar, G. Srivastava, G. P. Gupta, R. Tripathi, T. R.\\nGadekallu, and N. Xiong, “PPSF: A privacy-preserving and secure\\nframework using blockchain-based machine-learning for IoT-driven\\nsmart cities,” IEEE Transactions on Network Science and Engineering ,\\nvol. 8, no. 3, pp. 2326–2341, Jul.-Sep. 2021.\\n[97] M. A. Khan, S. Abbas, A. Rehman, Y . Saeed, A. Zeb, M. I. Uddin,\\nN. Nasser, and A. Ali, “A machine learning approach for blockchain-\\nbased smart home networks security,” IEEE Network , vol. 35, no. 3,\\npp. 223–229, May/Jun. 2021.\\n[98] J. Weng, J. Weng, J. Zhang, M. Li, Y . Zhang, and W. Luo, “DeepChain:\\nAuditable and privacy-preserving deep learning with blockchain-based\\nincentive,” IEEE Transactions on Dependable and Secure Computing ,\\nvol. 18, no. 5, pp. 2438–2455, Sep.-Oct. 2021.\\n[99] D. C. Nguyen, P. N. Pathirana, M. Ding, and A. Seneviratne, “Privacy-\\npreserved task ofﬂoading in mobile blockchain with deep reinforcement\\nlearning,” IEEE Transactions on Network and Service Management ,\\nvol. 17, no. 4, pp. 2536–2549, Dec. 2020.\\n[100] S. Wang, S. Sun, X. Wang, Z. Ning, and J. J. P. C. Rodrigues, “Secure\\ncrowdsensing in 5G internet of vehicles: When deep reinforcement\\nlearning meets blockchain,” IEEE Consumer Electronics Magazine ,\\nvol. 10, no. 5, pp. 72–81, Sep. 2021.\\n[101] Y . Lu, X. Huang, Y . Dai, S. Maharjan, and Y . Zhang, “Blockchain\\nand federated learning for privacy-preserved data sharing in industrial\\nIoT,” IEEE Transactions on Industrial Informatics , vol. 16, no. 6, pp.\\n4177–4186, Jun. 2020.\\n[102] S. Fan, H. Zhang, Y . Zeng, and W. Cai, “Hybrid blockchain-based\\nresource trading system for federated learning in edge computing,”\\nIEEE Internet of Things Journal , vol. 8, no. 4, pp. 2252–2264, Feb.\\n2021.[103] H. Liu, S. Zhang, P. Zhang, X. Zhou, X. Shao, G. Pu, and Y . Zhang,\\n“Blockchain and federated learning for collaborative intrusion detec-\\ntion in vehicular edge computing,” IEEE Transactions on Vehicular\\nTechnology , vol. 70, no. 6, pp. 6073–6084, Jun. 2021.\\n[104] P. Ocheja, B. Flanagan, and H. Ogata, “Connecting decentralized learn-\\ning records: a blockchain based learning analytics platform,” in Proc.\\n8th International Conference on Learning Analytics and Knowledge ,\\nSydney, New South Wales, Australia, Mar. 2018, pp. 265–269.\\n[105] U. Majeed, L. U. Khan, A. Yousafzai, Z. Han, B. J. Park, and\\nC. S. Hong, “ST-BFL: A structured transparency empowered cross-\\nsilo federated learning on the blockchain framework,” IEEE Access ,\\nvol. 9, pp. 155 634–155 650, 2021.\\n[106] J. Li, Y . Shao, K. Wei, M. Ding, C. Ma, L. Shi, Z. Han, and H. V .\\nPoor, “Blockchain assisted decentralized federated learning (BLADE-\\nFL): Performance analysis and resource allocation,” arXiv preprint\\narXiv:2101.06905 , 2021.\\n[107] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Artiﬁcial\\nneural networks-based machine learning for wireless networks: A\\ntutorial,” IEEE Communications Surveys & Tutorials , vol. 21, no. 4,\\npp. 3039–3071, Fourthquarter 2019.\\n[108] C. She, C. Sun, Z. Gu, Y . Li, C. Yang, H. V . Poor, and B. Vucetic,\\n“A tutorial on ultrareliable and low-latency communications in 6G:\\nIntegrating domain knowledge into deep learning,” Proceedings of the\\nIEEE , vol. 109, no. 3, pp. 204–246, Mar. 2021.\\n[109] C. She, R. Dong, Z. Gu, Z. Hou, Y . Li, W. Hardjawana, C. Yang,\\nL. Song, and B. Vucetic, “Deep learning for ultra-reliable and low-\\nlatency communications in 6G networks,” IEEE Network , vol. 34, no. 5,\\npp. 219–225, Sep./Oct. 2020.\\n[110] M. Alsenwi, N. H. Tran, M. Bennis, S. R. Pandey, A. K. Bairagi,\\nand C. S. Hong, “Intelligent resource slicing for eMBB and URLLC\\ncoexistence in 5G and beyond: A deep reinforcement learning based\\napproach,” IEEE Transactions on Wireless Communications , vol. 20,\\nno. 7, pp. 4585–4600, Jul. 2021.\\n[111] B. Gu, X. Zhang, Z. Lin, and M. Alazab, “Deep multiagent\\nreinforcement-learning-based resource allocation for internet of con-\\ntrollable things,” IEEE Internet of Things Journal , vol. 8, no. 5, pp.\\n3066–3074, Mar. 2021.\\n[112] A. Azari, M. Ozger, and C. Cavdar, “Risk-aware resource allocation\\nfor URLLC: Challenges and strategies with machine learning,” IEEE\\nCommunications Magazine , vol. 57, no. 3, pp. 42–48, Mar. 2019.\\n[113] T. Huynh-The, C.-H. Hua, Q.-V . Pham, and D.-S. Kim, “MCNet: An\\nefﬁcient CNN architecture for robust automatic modulation classiﬁca-\\ntion,” IEEE Communications Letters , vol. 24, no. 4, pp. 811–815, Apr.\\n2020.\\n[114] G. B. Tunze, T. Huynh-The, J.-M. Lee, and D.-S. Kim, “Sparsely\\nconnected CNN for efﬁcient automatic modulation recognition,” IEEE\\nTransactions on Vehicular Technology , vol. 69, no. 12, pp. 15 557–\\n15 568, Dec. 2020.\\n[115] C. Luo, J. Ji, Q. Wang, X. Chen, and P. Li, “Channel state information\\nprediction for 5G wireless communications: A deep learning approach,”\\nIEEE Transactions on Network Science and Engineering , vol. 7, no. 1,\\npp. 227–236, Jan.-Mar. 2020.\\n[116] S. Guo, Y . Lin, S. Li, Z. Chen, and H. Wan, “Deep spatial–temporal\\n3D convolutional neural networks for trafﬁc data forecasting,” IEEE\\nTransactions on Intelligent Transportation Systems , vol. 20, no. 10,\\npp. 3913–3926, Oct. 2019.\\n[117] F. Tao, H. Zhang, A. Liu, and A. Y . C. Nee, “Digital twin in industry:\\nState-of-the-art,” IEEE Transactions on Industrial Informatics , vol. 15,\\nno. 4, pp. 2405–2415, Apr. 2019.\\n[118] D. Chen, D. Wang, Y . Zhu, and Z. Han, “Digital twin for federated\\nanalytics using a bayesian approach,” IEEE Internet of Things Journal ,\\nvol. 8, no. 22, pp. 16 301–16 312, Nov. 2021.\\n[119] M. M. Rathore, S. A. Shah, D. Shukla, E. Bentafat, and S. Bakiras,\\n“The role of AI, machine learning, and big data in digital twinning:\\nA systematic literature review, challenges, and opportunities,” IEEE\\nAccess , vol. 9, pp. 32 030–32 052, 2021.\\n[120] H. Darvishi, D. Ciuonzo, E. R. Eide, and P. S. Rossi, “Sensor-fault\\ndetection, isolation and accommodation for digital twins via modular\\ndata-driven architecture,” IEEE Sensors Journal , vol. 21, no. 4, pp.\\n4827–4838, Feb. 2021.\\n[121] Q. Wang, W. Jiao, P. Wang, and Y . Zhang, “Digital twin for human-\\nrobot interactive welding and welder behavior analysis,” IEEE/CAA\\nJournal of Automatica Sinica , vol. 8, no. 2, pp. 334–343, Feb. 2021.\\n[122] H. Elayan, M. Aloqaily, and M. Guizani, “Digital twin for intelligent\\ncontext-aware IoT healthcare systems,” IEEE Internet of Things Jour-\\nnal, vol. 8, no. 23, pp. 16 749–16 757, Dec. 2021.23\\n[123] A. Ghandar, A. Ahmed, S. Zulﬁqar, Z. Hua, M. Hanai, and G. Theodor-\\nopoulos, “A decision support system for urban agriculture using digital\\ntwin: A case study with aquaponics,” IEEE Access , vol. 9, pp. 35 691–\\n35 708, 2021.\\n[124] X. Xu, B. Shen, S. Ding, G. Srivastava, M. Bilal, M. R. Khosravi,\\nV . G. Menon, M. A. Jan, and M. Wang, “Service ofﬂoading with\\ndeep Q-network for digital twinning-empowered internet of vehicles in\\nedge computing,” IEEE Transactions on Industrial Informatics , vol. 18,\\nno. 2, pp. 1414–1423, Feb. 2022.\\n[125] R. Dong, C. She, W. Hardjawana, Y . Li, and B. Vucetic, “Deep learning\\nfor hybrid 5G services in mobile edge computing systems: Learn from a\\ndigital twin,” IEEE Transactions on Wireless Communications , vol. 18,\\nno. 10, pp. 4692–4707, Oct. 2019.\\n[126] W. Sun, S. Lei, L. Wang, Z. Liu, and Y . Zhang, “Adaptive federated\\nlearning and digital twin for industrial internet of things,” IEEE\\nTransactions on Industrial Informatics , vol. 17, no. 8, pp. 5605–5614,\\nAug. 2021.\\n[127] S. L. Bernal, A. H. Celdr ´an, G. M. P ´erez, M. T. Barros, and S. Bala-\\nsubramaniam, “Security in brain-computer interfaces: State-of-the-art,\\nopportunities, and future challenges,” ACM Computing Surveys , vol. 54,\\nno. 1, pp. 1–35, Jan. 2022.\\n[128] H. He and D. Wu, “Transfer learning for brain–computer interfaces:\\nA euclidean space data alignment approach,” IEEE Transactions on\\nBiomedical Engineering , vol. 67, no. 2, pp. 399–410, Feb. 2020.\\n[129] B. Abibullaev and A. Zollanvari, “Learning discriminative spatiospec-\\ntral features of ERPs for accurate brain–computer interfaces,” IEEE\\nJournal of Biomedical and Health Informatics , vol. 23, no. 5, pp. 2009–\\n2020, Sep. 2019.\\n[130] A. Matran-Fernandez and R. Poli, “Brain–computer interfaces for de-\\ntection and localization of targets in aerial images,” IEEE Transactions\\non Biomedical Engineering , vol. 64, no. 4, pp. 959–969, Apr. 2017.\\n[131] Z. Lv, L. Qiao, Q. Wang, and F. Piccialli, “Advanced machine-learning\\nmethods for brain-computer interfacing,” IEEE/ACM Transactions on\\nComputational Biology and Bioinformatics , vol. 18, no. 5, pp. 1688–\\n1698, 2021.\\n[132] R. Ma, T. Yu, X. Zhong, Z. L. Yu, Y . Li, and Z. Gu, “Capsule network\\nfor ERP detection in brain-computer interface,” IEEE Transactions on\\nNeural Systems and Rehabilitation Engineering , vol. 29, pp. 718–730,\\nApr. 2021.\\n[133] S. Sakhavi, C. Guan, and S. Yan, “Learning temporal information for\\nbrain-computer interface using convolutional neural networks,” IEEE\\nTransactions on Neural Networks and Learning Systems , vol. 29,\\nno. 11, pp. 5619–5629, Nov. 2018.\\n[134] J.-H. Jeong, K.-H. Shim, D.-J. Kim, and S.-W. Lee, “Brain-controlled\\nrobotic arm system based on multi-directional CNN-BiLSTM network\\nusing EEG signals,” IEEE Transactions on Neural Systems and Reha-\\nbilitation Engineering , vol. 28, no. 5, pp. 1226–1238, May 2020.\\n[135] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper with\\nconvolutions,” in Proc. IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR) , Boston, MA, Jun. 2015, pp. 1–9.\\n[136] E. Santamar ´ıa-V´azquez, V . Mart ´ınez-Cagigal, F. Vaquerizo-Villar, and\\nR. Hornero, “EEG-Inception: A novel deep convolutional neural net-\\nwork for assistive ERP-based brain-computer interfaces,” IEEE Trans-\\nactions on Neural Systems and Rehabilitation Engineering , vol. 28,\\nno. 12, pp. 2773–2782, Dec. 2020.\\n[137] O. Banos, J. Bang, T. Hur, M. H. Siddiqi, H.-T. Thien, L.-B. Vui,\\nW. Ali Khan, T. Ali, C. Villalonga, and S. Lee, “Mining human\\nbehavior for health promotion,” in Proc. 37th Annual International\\nConference of the IEEE Engineering in Medicine and Biology Society\\n(EMBC) , Milan, Italy, Aug. 2015, pp. 5062–5065.\\n[138] T. Huynh-The, C.-H. Hua, N. A. Tu, and D.-S. Kim, “Physical activity\\nrecognition with statistical-deep fusion model using multiple sensory\\ndata for smart health,” IEEE Internet of Things Journal , vol. 8, no. 3,\\npp. 1533–1543, Feb. 2021.\\n[139] T. Hur, J. Bang, J. Lee, J.-I. Kim, S. Lee et al. , “Iss2Image: A novel\\nsignal-encoding technique for CNN-based human activity recognition,”\\nSensors , vol. 18, no. 11, p. 3910, Nov. 2018.\\n[140] X. Qian, H. Chen, H. Jiang, J. Green, H. Cheng, and M.-C. Huang,\\n“Wearable computing with distributed deep learning hierarchy: A study\\nof fall detection,” IEEE Sensors Journal , vol. 20, no. 16, pp. 9408–\\n9416, Aug. 2020.\\n[141] H. Li, A. Shrestha, H. Heidari, J. Le Kernec, and F. Fioranelli, “Bi-\\nLSTM network for multimodal continuous human activity recognition\\nand fall detection,” IEEE Sensors Journal , vol. 20, no. 3, pp. 1191–\\n1201, Feb. 2020.[142] C.-H. Hua, T. Huynh-The, K. Kim, S.-Y . Yu, T. Le-Tien, G. H. Park,\\nJ. Bang, W. A. Khan, S.-H. Bae, and S. Lee, “Bimodal learning via\\ntrilogy of skip-connection deep networks for diabetic retinopathy risk\\nprogression identiﬁcation,” International Journal of Medical Informat-\\nics, vol. 132, p. 103926, Dec. 2019.\\n[143] C.-H. Hua, K. Kim, T. Huynh-The, J. I. You, S.-Y . Yu, T. Le-Tien,\\nS.-H. Bae, and S. Lee, “Convolutional network with twofold feature\\naugmentation for diabetic retinopathy recognition from multi-modal\\nimages,” IEEE Journal of Biomedical and Health Informatics , vol. 25,\\nno. 7, pp. 2686–2697, Jul. 2021.\\n[144] Z. Ning, S. Zhong, Q. Feng, W. Chen, and Y . Zhang, “SMU-Net:\\nSaliency-guided morphology-aware U-Net for breast lesion segmen-\\ntation in ultrasound image,” IEEE Transactions on Medical Imaging ,\\nvol. 41, no. 2, pp. 476–490, Feb. 2022.\\n[145] H. Qi, N. Fuin, G. Cruz, J. Pan, T. Kuestner, A. Bustin, R. M. Botnar,\\nand C. Prieto, “Non-rigid respiratory motion estimation of whole-\\nheart coronary MR images using unsupervised deep learning,” IEEE\\nTransactions on Medical Imaging , vol. 40, no. 1, pp. 444–454, Jan.\\n2021.\\n[146] T. Hassanzadeh, D. Essam, and R. Sarker, “2D to 3D evolutionary deep\\nconvolutional neural networks for medical image segmentation,” IEEE\\nTransactions on Medical Imaging , vol. 40, no. 2, pp. 712–721, Feb.\\n2021.\\n[147] J. Torner, S. Skouras, J. L. Molinuevo, J. D. Gispert, and F. Alpiste,\\n“Multipurpose virtual reality environment for biomedical and health\\napplications,” IEEE Transactions on Neural Systems and Rehabilitation\\nEngineering , vol. 27, no. 8, pp. 1511–1520, Aug. 2019.\\n[148] S. Doltsinis, P. Ferreira, and N. Lohse, “A symbiotic human–machine\\nlearning approach for production ramp-up,” IEEE Transactions on\\nHuman-Machine Systems , vol. 48, no. 3, pp. 229–240, Jun. 2018.\\n[149] M. Azamfar, X. Li, and J. Lee, “Deep learning-based domain adaptation\\nmethod for fault diagnosis in semiconductor manufacturing,” IEEE\\nTransactions on Semiconductor Manufacturing , vol. 33, no. 3, pp. 445–\\n453, Aug. 2020.\\n[150] H. Lee, H. Jeong, G. Koo, J. Ban, and S. W. Kim, “Attention\\nrecurrent neural network-based severity estimation method for interturn\\nshort-circuit fault in permanent magnet synchronous machines,” IEEE\\nTransactions on Industrial Electronics , vol. 68, no. 4, pp. 3445–3453,\\nApr. 2021.\\n[151] Z. Y . Xue, K. S. Xiahou, M. S. Li, T. Y . Ji, and Q. H. Wu, “Diagnosis\\nof multiple open-circuit switch faults based on long short-term memory\\nnetwork for DFIG-based wind turbine systems,” IEEE Journal of\\nEmerging and Selected Topics in Power Electronics , vol. 8, no. 3, pp.\\n2600–2610, Sep. 2020.\\n[152] L. Guo, Y . Lei, S. Xing, T. Yan, and N. Li, “Deep convolutional\\ntransfer learning network: A new method for intelligent fault diagnosis\\nof machines with unlabeled data,” IEEE Transactions on Industrial\\nElectronics , vol. 66, no. 9, pp. 7316–7325, Sep. 2019.\\n[153] H. Alkhalefah, J. E. A. Qudeiri, U. Umer, M. H. Abidi, and A. Elkaseer,\\n“Development of an efﬁcient prediction model for optimal design of\\nserial production lines,” IEEE Access , vol. 9, pp. 61 807–61 818, 2021.\\n[154] S. Huang, Y . Guo, D. Liu, S. Zha, and W. Fang, “A two-stage\\ntransfer learning-based deep learning approach for production progress\\nprediction in IoT-enabled manufacturing,” IEEE Internet of Things\\nJournal , vol. 6, no. 6, pp. 10 627–10 638, Dec. 2019.\\n[155] R. G. Lins and S. N. Givigi, “Cooperative robotics and machine\\nlearning for smart manufacturing: Platform design and trends within\\nthe context of industrial internet of things,” IEEE Access , vol. 9, pp.\\n95 444–95 455, 2021.\\n[156] V . Kohli, U. Tripathi, V . Chamola, B. K. Rout, and S. S. Kanhere,\\n“A review on virtual reality and augmented reality use-cases of brain\\ncomputer interface based applications for smart cities,” Microproces-\\nsors and Microsystems , vol. 88, p. 104392, Feb. 2022.\\n[157] M. Mohammadi and A. Al-Fuqaha, “Enabling cognitive smart cities\\nusing big data and machine learning: Approaches and challenges,”\\nIEEE Communications Magazine , vol. 56, no. 2, pp. 94–101, Feb. 2018.\\n[158] S. Park, H.-S. Cha, and C.-H. Im, “Development of an online home\\nappliance control system using augmented reality and an SSVEP-based\\nbrain–computer interface,” IEEE Access , vol. 7, pp. 163 604–163 614,\\n2019.\\n[159] F. Zhu, Y . Lv, Y . Chen, X. Wang, G. Xiong, and F.-Y . Wang, “Parallel\\ntransportation systems: Toward IoT-enabled smart urban trafﬁc control\\nand management,” IEEE Transactions on Intelligent Transportation\\nSystems , vol. 21, no. 10, pp. 4063–4071, Oct. 2020.\\n[160] S. Du, T. Li, Y . Yang, and S.-J. Horng, “Deep air quality forecasting\\nusing hybrid deep learning framework,” IEEE Transactions on Knowl-\\nedge and Data Engineering , vol. 33, no. 6, pp. 2412–2424, Jun. 2021.24\\n[161] S. A. Bhat and N.-F. Huang, “Big data and AI revolution in precision\\nagriculture: Survey and challenges,” IEEE Access , vol. 9, pp. 110 209–\\n110 222, 2021.\\n[162] G. Synnaeve and P. Bessi `ere, “Multiscale bayesian modeling for\\nRTS games: An application to StarCraft AI,” IEEE Transactions on\\nComputational Intelligence and AI in Games , vol. 8, no. 4, pp. 338–\\n350, Dec. 2016.\\n[163] A. Liaqat, M. A. Sindhu, and G. F. Siddiqui, “Metamorphic testing of\\nan artiﬁcially intelligent chess game,” IEEE Access , vol. 8, pp. 174 179–\\n174 190, 2020.\\n[164] I. Oh, S. Rho, S. Moon, S. Son, H. Lee, and J. Chung, “Creating\\npro-level AI for a real-time ﬁghting game using deep reinforcement\\nlearning,” IEEE Transactions on Games , Jan. 2021, (Early Access).\\n[165] N. A. Barriga, M. Stanescu, F. Besoain, and M. Buro, “Improving\\nRTS game AI by supervised policy learning, tactical search, and deep\\nreinforcement learning,” IEEE Computational Intelligence Magazine ,\\nvol. 14, no. 3, pp. 8–18, Aug. 2019.\\n[166] G. N. Yannakakis and J. Togelius, “A panorama of artiﬁcial and com-\\nputational intelligence in games,” IEEE Transactions on ComputationalIntelligence and AI in Games , vol. 7, no. 4, pp. 317–335, Dec. 2015.\\n[167] M. Frutos-Pascual and B. G. Zapirain, “Review of the use of ai\\ntechniques in serious games: Decision making and machine learning,”\\nIEEE Transactions on Computational Intelligence and AI in Games ,\\nvol. 9, no. 2, pp. 133–152, Jun. 2017.\\n[168] E. Riedl, “A look into e-commerce and more in the metaverse,”\\nSep. 2021. [Online]. Available: https://www.mastercard.com/news/\\nperspectives/2021/ecommerce-metaverse-augmented-mixed-reality/\\n[169] L. Handley, “Looking for a job? you might get\\nhired via the metaverse, experts say,” Nov. 2021.\\n[Online]. Available: https://www.cnbc.com/2021/11/30/\\nlooking-for-a-job-you-might-get-hired-via-the-metaverse-experts-say.\\nhtml\\n[170] Meta, “Introducing horizon workrooms: Remote collaboration reimag-\\nined,” Aug. 2021. [Online]. Available: https://about.fb.com/news/2021/\\n08/introducing-horizon-workrooms-remote-collaboration-reimagined/\\n[171] D. Kamin, “Investors snap up metaverse real estate in a virtual\\nland boom,” Nov. 2021. [Online]. Available: https://www.nytimes.\\ncom/2021/11/30/business/metaverse-real-estate.html'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the tetc using Character Text Split Such that it should not increase token size \n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading embedding from OpenAI\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_search= FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x23a111b8be0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' According to the Bloomberg Intelligence [1], the global metaverse revenue opportunity will increase from USD 500 billion in 2020 to USD 800billion in 2024, with the software and services revenue increasing from $183.3B to $250.5B.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How much software and services revenue will be increased by\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Snoop Dogg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who bought a plot land in metaverse\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Metaverse is not a new idea, it has circulated along with the development of the Internet and other technologies for decades.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Why is metaverse a new idea\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
